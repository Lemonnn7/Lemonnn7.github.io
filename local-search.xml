<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MAVEN教程</title>
    <link href="/MAVEN%E6%95%99%E7%A8%8B.html"/>
    <url>/MAVEN%E6%95%99%E7%A8%8B.html</url>
    
    <content type="html"><![CDATA[<h1 id="maven笔记">Maven笔记</h1><h2 id="maven简介">Maven简介</h2><p>Maven的本质是一个项目管理工具，将项目开发和管理过程抽象成一个项目对象模型(POM)<br />POM (Project Object Model)：项目对象模型<img src="https://gitee.com/HXBISHERE/img_bed/raw/master/img/截屏2021-07-17 下午6.51.07.png" alt="截屏2021-07-17 下午6.51.07" style="zoom:50%;" /></p><p>Maven的作用：</p><ol type="1"><li>项目构建：提供标准的、跨平台的自动化项目构建方式</li><li>依赖管理：方便快捷的管理项目依赖的资源（jar包），避免资源间的版本冲突问题</li><li>统一开发结构：提供标准的、统一的项目结构</li></ol><h2 id="maven基础概念">Maven基础概念</h2><p>仓库：用于存储资源，包含各种jar包仓库分类：本地仓库和远程仓库（私服和中央仓库）</p><p>坐标：Maven中的坐标用于描述仓库中资源的位置 坐标的主要组成：groupId：定义当前Maven项目隶属组织名称（通常是域名反写）artifactId：定义当前Maven项目名称（通常是模块名称）version：定义当前版本号 packaging：定义该项目的打包方式坐标的作用：使用唯一的标识，唯一性定位资源位置，通过该标识可以将资源的识别与下载交由机器完成。</p><p>MVAEN环境配置:</p><p>首先下载好相应的MAVEN版本解压到指定位置</p><p>1.添加系统变量</p><figure><imgsrc="https://gitee.com/HXBISHERE/img_bed/raw/master/img/image-20220121113726049.png"alt="image-20220121113726049" /><figcaption aria-hidden="true">image-20220121113726049</figcaption></figure><p>2.在path中添加</p><figure><imgsrc="https://gitee.com/HXBISHERE/img_bed/raw/master/img/image-20220121113829888.png"alt="image-20220121113829888" /><figcaption aria-hidden="true">image-20220121113829888</figcaption></figure><p>仓库配置：</p><p>打开【maven】目录 -&gt;【conf】-&gt; 【setting.xml】</p><p>本地仓库配置：默认位置与自定义位置</p><p>MAVEN启动后会自动保存下载的资源到本地仓库</p><ul><li>默认位置:</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">localRepository</span>&gt;</span>$&#123;user.home&#125;/.m2/repository<span class="hljs-tag">&lt;/<span class="hljs-name">localRepository</span>&gt;</span><br></code></pre></td></tr></table></figure><p>当前目录位置为登录用户名所在的目录下的.m2文件夹中</p><ul><li>自定义位置:</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">localRepository</span>&gt;</span>D:\maven\repository<span class="hljs-tag">&lt;/<span class="hljs-name">localRepository</span>&gt;</span><br></code></pre></td></tr></table></figure><p>当前目录位置为D:</p><p>远程仓库配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">repositories</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">repository</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>central<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>Central Repository<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <br>    <span class="hljs-tag">&lt;<span class="hljs-name">url</span>&gt;</span>https://repo.maven.apache.org/maven2<span class="hljs-tag">&lt;/<span class="hljs-name">url</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">layout</span>&gt;</span>default<span class="hljs-tag">&lt;/<span class="hljs-name">layout</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">snapshots</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">enabled</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">enabled</span>&gt;</span> <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">snapshots</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">repository</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">repositories</span>&gt;</span><br></code></pre></td></tr></table></figure><p>镜像仓库配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">mirrors</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">mirror</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 此镜像的唯一标识符，用来区分不同的mirror元素 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>nexus-aliyun<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 对那种仓库进行镜像（就是替代哪种仓库）--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">mirrorOf</span>&gt;</span>central<span class="hljs-tag">&lt;/<span class="hljs-name">mirrorOf</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 镜像名称 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>Nexus aliyun<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 镜像URL --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="hljs-tag">&lt;/<span class="hljs-name">url</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">mirror</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">mirrors</span>&gt;</span><br><br>更新目前可用镜像源（2022/1/21）：<br><span class="hljs-tag">&lt;<span class="hljs-name">mirror</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>alimaven<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">mirrorOf</span>&gt;</span>central<span class="hljs-tag">&lt;/<span class="hljs-name">mirrorOf</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>aliyun maven<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/repositories/central/<span class="hljs-tag">&lt;/<span class="hljs-name">url</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">mirror</span>&gt;</span><br></code></pre></td></tr></table></figure><h2 id="maven项目">Maven项目</h2><h3 id="手动生成maven项目">手动生成Maven项目</h3><p>Maven工程目录结构<img src="https://gitee.com/HXBISHERE/img_bed/raw/master/img/截屏2021-07-17 下午6.50.36.png" alt="截屏2021-07-17 下午6.50.36" style="zoom:50%;" /></p><p>Maven项目构建命令Maven构建命令使用mvn开头，后面加功能参数，可以一次执行多个命令，使用空格分隔</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Maven">mvn compile #编译<br>mvn clean #清理<br>mvn test #测试<br>mvn package#打包<br>mvn install #安装到本地仓库<br></code></pre></td></tr></table></figure><h3 id="idea生成maven项目">IDEA生成Maven项目</h3><p>使用原型创建Maven项目与不使用原型创建Maven项目例：使用原型创建web项目，选择archetype-webapp进行项目创建，添加Tomcat插件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">build</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">plugins</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span> <br>      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.tomcat.maven<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span> <br>      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>tomcat7-maven-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span> <br>      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.1<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>            <span class="hljs-comment">&lt;!--隐藏路径和端口设置--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">port</span>&gt;</span>80<span class="hljs-tag">&lt;/<span class="hljs-name">port</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">path</span>&gt;</span>/<span class="hljs-tag">&lt;/<span class="hljs-name">path</span>&gt;</span>  <br>      <span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span> <br>  <span class="hljs-tag">&lt;/<span class="hljs-name">plugins</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">build</span>&gt;</span><br></code></pre></td></tr></table></figure><h2 id="依赖管理">依赖管理</h2><ol type="1"><li><p>依赖配置</p><p>依赖指的是当前项目运行所需要的jar，一个项目可以设置多个依赖格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--设置当前项目所依赖的所有jar--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><br>  <span class="hljs-comment">&lt;!--设置具体的依赖--&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--依赖所属群组id--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--依赖所属项目id--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--依赖版本号--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>依赖传递</p><p>依赖具有传递性，包括直接传递和间接传递。直接传递：在当前项目中通过依赖配置建立的依赖关系（A使用B，A和B就是直接传递）间接传递：被依赖的资源如果依赖其他资源，当前项目间接依赖其他资源（比较拗口，意思是如果A依赖B，而B依赖C，那么A和C之间就是间接传递）</p><p>依赖传递的冲突问题：路径优先：当依赖中出现相同的资源时，层级越深，优先级越低，层级越浅，优先级越高声明优先：当资源在相同层级被依赖时，配置顺序靠前的覆盖配置顺序靠后的特殊优先：当同级配置了相同资源的不同版本，后配置的覆盖先配置的</p><p><img src="https://gitee.com/HXBISHERE/img_bed/raw/master/img/截屏2021-07-17 下午6.49.40.png" alt="截屏2021-07-17 下午6.49.40" style="zoom:33%;" /></p></li><li><p>可选依赖</p><p>可选依赖指的是对外隐藏当前所依赖的资源</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>  <span class="hljs-comment">&lt;!--添加下面这一行--&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">optional</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">optional</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>排除依赖</p><p>排除依赖指主动断开依赖的资源，被排除的资源无需指定版本</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">exclusions</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">exclusion</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">exclusion</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">exclusions</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>依赖范围</p><p>依赖的jar包默认情况可以在任何地方使用，可以通过scope标签设定其作用范围作用范围： 主程序范围有效（main文件夹范围内）测试程序范围有效（test文件夹范围内）是否参与打包（package文件夹范围内）</p><p><img src="https://gitee.com/HXBISHERE/img_bed/raw/master/img/截屏2021-07-17 下午7.10.15.png" alt="截屏2021-07-17 下午7.10.15" style="zoom:50%;" /></p></li></ol><h2 id="生命周期与插件">生命周期与插件</h2><p>Maven项目构建生命周期描述的是一次构建过程经历了多少个事件Maven对项目构建的生命周期划分为3套 clean：清理工作default：核心工作，例如编译、测试、打包、部署等 compile -- test-compile-- test -- packege -- Install<br />site：产生报告，发布站点等</p><p>插件：插件与生命周期内的阶段绑定，在执行到对应的生命周期时执行对应的插件功能</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">build</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">plugins</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span> <br>      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span> <br>      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>maven-source-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span> <br>      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.2.1<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">executions</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">execution</span>&gt;</span> <br>          <span class="hljs-tag">&lt;<span class="hljs-name">goals</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">goal</span>&gt;</span>jar<span class="hljs-tag">&lt;/<span class="hljs-name">goal</span>&gt;</span> <br>          <span class="hljs-tag">&lt;/<span class="hljs-name">goals</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">phase</span>&gt;</span> generate-test-resources<span class="hljs-tag">&lt;/<span class="hljs-name">phase</span>&gt;</span> <span class="hljs-tag">&lt;/<span class="hljs-name">execution</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">executions</span>&gt;</span> <br>    <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">plugins</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">build</span>&gt;</span><br></code></pre></td></tr></table></figure><h2 id="分模块开发与设计">分模块开发与设计</h2><p>参考视频进行实验即可</p><h2 id="聚合">聚合</h2><p>作用：聚合用于快速构建Maven工程，一次性构建多个项目/模块。制作方式：创建一个空模块，打包类型定义为pom</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">packaging</span>&gt;</span>pom<span class="hljs-tag">&lt;/<span class="hljs-name">packaging</span>&gt;</span><br></code></pre></td></tr></table></figure><p>定义当前模块进行构建操作时关联的其他模块名称</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">modules</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>模块地址<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>模块地址<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>模块地址<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">module</span>&gt;</span>模块地址<span class="hljs-tag">&lt;/<span class="hljs-name">module</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">modules</span>&gt;</span><br></code></pre></td></tr></table></figure><p>注意：参与聚合操作的模块最终执行顺序与模块间的依赖关系有关，与配置顺序无关</p><h2 id="继承">继承</h2><p>作用：通过继承可以实现在子工程中沿用父工程中的配置（与Java类似）制作方式：在子工程中生命其父工程坐标与对应的位置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--定义该工程的父工程--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">parent</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>  <span class="hljs-comment">&lt;!--填写父工程的pom文件--&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">relativePath</span>&gt;</span>父工程pom文件地址<span class="hljs-tag">&lt;/<span class="hljs-name">relativePath</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">parent</span>&gt;</span><br></code></pre></td></tr></table></figure><p>在父工程中定义依赖管理</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--声明此处进行依赖管理--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">dependencyManagement</span>&gt;</span><br>  <span class="hljs-comment">&lt;!--具体的依赖--&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependencyManagement</span>&gt;</span><br></code></pre></td></tr></table></figure><p>继承依赖使用：在子工程中定义依赖关系，无需声明依赖版本，版本参照父工程中依赖的版本</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="继承与聚合">继承与聚合</h3><p>作用：聚合用于快速构建项目，继承用于快速配置 相同点：聚合与继承的pom.xml文件打包方式均为pom，可以将两种关系制作到同一个pom文件中聚合与继承均属于设计型模块，并无实际的模块内容不同点：聚合是在当前模块中配置关系，聚合可以感知到参与聚合的模块有哪些继承是在子模块中配置关系，父模块无法感知哪些子模块继承了自己</p><h2 id="属性">属性</h2><ol type="1"><li><p>自定义属性</p><p>作用：等同于定义变量，方便统一维护 定义格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--定义自定义属性--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">spring.version</span>&gt;</span>5.1.9.RELEASE<span class="hljs-tag">&lt;/<span class="hljs-name">spring.version</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">junit.version</span>&gt;</span>4.12<span class="hljs-tag">&lt;/<span class="hljs-name">junit.version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span><br></code></pre></td></tr></table></figure><p>调用格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.springframework<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spring-context<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span> <br>  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$&#123;spring.version&#125;<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p>内置属性</p><p>作用：使用Maven内置属性，快速配置 调用格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs xml">$&#123;basedir&#125;<br>$&#123;version&#125;<br></code></pre></td></tr></table></figure></li><li><p>Setting属性</p><p>作用：使用Maven配置文件setting.xml中的标签属性，用于动态配置调用格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">$&#123;settings.localRepository&#125;<br></code></pre></td></tr></table></figure></li><li><p>Java系统属性</p><p>作用：读取Java系统属性 调用格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">$&#123;user.home&#125;<br></code></pre></td></tr></table></figure><p>系统属性查询方式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">mvn help:system<br></code></pre></td></tr></table></figure></li><li><p>环境变量属性</p><p>作用：使用Maven配置文件setting.xml中的标签属性，用于动态配置调用格式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">$&#123;env.JAVA_HOME&#125;<br></code></pre></td></tr></table></figure><p>环境变量属性查询方式：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">mvn help:system<br></code></pre></td></tr></table></figure></li></ol><h2 id="版本管理">版本管理</h2><p>SNAPSHOT（快照版本） RELEASE（发布版本） 工程版本号约定</p><p><img src="https://gitee.com/HXBISHERE/img_bed/raw/master/img/截屏2021-07-17 下午8.24.46.png" alt="截屏2021-07-17 下午8.24.46" style="zoom:50%;" /></p><h2 id="资源配置">资源配置</h2><p>配置文件引用pom属性 作用：在任意配置文件中加载pom文件中定义的属性调用格式</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">$&#123;地址&#125;<br></code></pre></td></tr></table></figure><p>开启配置文件加载pom属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--配置资源文件对应的信息--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">resources</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">resource</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--设定配置文件对应的位置目录，支持使用属性动态设定路径--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">directory</span>&gt;</span>地址<span class="hljs-tag">&lt;/<span class="hljs-name">directory</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--开启对配置文件的资源加载过滤--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">filtering</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">filtering</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">resource</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">resources</span>&gt;</span><br></code></pre></td></tr></table></figure><h2 id="多环境开发配置">多环境开发配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--创建多环境--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">profiles</span>&gt;</span><br>  <span class="hljs-comment">&lt;!--定义具体的环境：生产环境--&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">profile</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--定义环境对应的唯一名称--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>开发环境名称1<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--定义环境中的专用的属性值--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">jdbc.url</span>&gt;</span>jdbc链接<span class="hljs-tag">&lt;/<span class="hljs-name">jdbc.url</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">activation</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">activeByDefault</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">activeByDefault</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">activation</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">profile</span>&gt;</span><br>  <span class="hljs-comment">&lt;!--定义具体的环境：开发环境--&gt;</span><br>  <span class="hljs-comment">&lt;!--格式同上--&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">profiles</span>&gt;</span><br></code></pre></td></tr></table></figure><h2 id="私服">私服</h2><p>使用Nexus，按照视频进行操作即可</p><p>ps：相关文档见个人百度网盘</p>]]></content>
    
    
    <categories>
      
      <category>JAVA</category>
      
      <category>环境配置</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JAVA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特征选择（Feature selection）方法汇总</title>
    <link href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-selection%EF%BC%89%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB.html"/>
    <url>/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-selection%EF%BC%89%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB.html</url>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p><code>特征选择</code>是<code>特征工程</code>里的一个重要问题，其目标是<strong>寻找最优特征子集</strong>。特征选择能剔除不相关(irrelevant)或冗余(redundant)的特征，从而达到减少特征个数，<strong>提高模型精确度，减少运行时间的目的</strong>。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。</p><p>之所以要考虑特征选择，是因为机器学习经常面临过拟合的问题。<strong>过拟合</strong>的表现是模型参数<strong>太贴合训练集数据</strong>，模型在训练集上效果很好而在测试集上表现不好，也就是在高方差。简言之模型的泛化能力差。过拟合的原因是模型对于训练集数据来说太复杂，要解决过拟合问题，一般考虑如下方法：</p><ol type="1"><li>收集更多数据</li><li>通过正则化引入对复杂度的惩罚</li><li>选择更少参数的简单模型</li><li>对数据降维（降维有两种方式：特征选择和特征抽取）</li></ol><p>其中第1条一般是很难做到的，一般主要采用第2和第4点</p><h2 id="一般流程">一般流程</h2><p>特征选择的一般过程：</p><ol type="1"><li>生成子集：搜索特征子集，为评价函数提供特征子集</li><li>评价函数：评价特征子集的好坏</li><li>停止准则：与评价函数相关，一般是阈值，评价函数达到一定标准后就可停止搜索</li><li>验证过程：在验证数据集上验证选出来的特征子集的有效性</li></ol><p>但是， 当特征数量很大的时候，这个搜索空间会很大，如何找最优特征还是需要一些经验结论。</p><h2 id="三大类方法">三大类方法</h2><p>根据特征选择的形式，可分为三大类：</p><ul><li>Filter(过滤法)：按照<code>发散性</code>或<code>相关性</code>对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选</li><li>Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征</li><li>Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）</li></ul><h2 id="过滤法">过滤法</h2><p>基本想法是：分别对每个特征 <span class="math inline">\(x_i\)</span>，计算 <span class="math inline">\(x_i\)</span> 相对于类别标签 y的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前k个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量S(i) ，我们的目标是选取与 y 关联最密切的一些 特征<spanclass="math inline">\(x_i\)</span> 。</p><ul><li>Pearson相关系数</li><li>卡方验证</li><li>互信息和最大信息系数</li><li>距离相关系数</li><li>方差选择法</li></ul><h3 id="pearson相关系数">Pearson相关系数</h3><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，衡量的是变量之间的线性相关性，结果的取值区间为<strong>[-1,1]</strong>， -1 表示完全的负相关(这个变量下降，那个就会上升)， +1表示完全的正相关， 0 表示没有线性相关性。PearsonCorrelation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的<ahref="https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html">pearsonr</a>方法能够同时计算相关系数和p-value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> pearsonr<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>size = <span class="hljs-number">300</span><br>x = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Lower noise：&quot;</span>, pearsonr(x, x + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Higher noise：&quot;</span>, pearsonr(x, x + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size)))<br><br><br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-comment"># 选择K个最好的特征，返回选择特征后的数据</span><br><span class="hljs-comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数</span><br><span class="hljs-comment"># 参数k为选择的特征个数</span><br>SelectKBest(<span class="hljs-keyword">lambda</span> X, Y: array(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><p>Pearson相关系数的一个<strong>明显缺陷</strong>是，作为特征排序机制，他<strong>只对线性关系敏感</strong>。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0 。</p><h3 id="卡方验证">卡方验证</h3><p>经典的卡方检验是检验<strong>类别型变量</strong>对<strong>类别型变量</strong>的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：</p><p><span class="math display">\[χ2=∑(A−E)2E\]</span></p><p>不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用sklearn中feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> chi2<br>iris = load_iris()<br>X, y = iris.data, iris.target  <span class="hljs-comment">#iris数据集</span><br><br><span class="hljs-comment">#选择K个最好的特征，返回选择特征后的数据</span><br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><p><ahref="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/classes.html%23module-sklearn.feature_selection">sklearn.feature_selection</a>模块中的类可以用于样本集中的特征选择/维数降低，以提高估计器的准确度分数或提高其在非常高维数据集上的性能</p><h3id="互信息和最大信息系数-mutual-information-and-maximal-information-coefficient-mic">互信息和最大信息系数Mutual information and maximal information coefficient (MIC)</h3><p>经典的互信息也是评价<strong>类别型变量</strong>对<strong>类别型变量</strong>的相关性的，互信息公式如下：</p><p><span class="math display">\[MI(x_i,y)=∑x_i∈0,1∑y∈0,1\  p(x_i,y)logp(x_i,y)p(x_i)p(y)\]</span></p><p>当<spanclass="math inline">\(x_i\)</span>是0/1离散值的时候，这个公式如上。很容易推广到<span class="math inline">\(x_i\)</span> 是多个离散值的情况。这里的<span class="math inline">\(p(x_i,y) , p(x_i)\)</span> 和 p(y)都是从训练集上得到的。若问这个 MI 公式如何得来，请看它的 KL距离（Kullback-Leibler）表述： $MI(x_i,y)=KL(P(x_i,y)||p(x_i)p(y))$也就是说, MI 衡量的是 <span class="math inline">\(x_i\)</span> 和 <spanclass="math inline">\(y\)</span> 的独立性。如果它俩独立 <spanclass="math inline">\(P(x_i,y)=p(x_i)p(y)\)</span> ，那么 KL距离值为0，也就是 <span class="math inline">\(x_i\)</span> 和 <spanclass="math inline">\(y\)</span> 不相关了，可以去除 <spanclass="math inline">\(x_i\)</span>。相反，如果两者密切相关，那么 MI值会很大。在对 MI 进行排名后，最后剩余的问题就是如何选择 k 个值（前 k 个<span class="math inline">\(x_i\)</span>）。(后面将会提到此方法)我们继续使用交叉验证的方法，将 k 从 1 扫描到 n，取评分最高的k 。不过这次复杂度是线性的了。比如，在使用朴素贝叶斯分类文本的时候，词表长度n 很大。 使用filiter特征选择方法，能够增加分类器精度。</p><p>想把互信息直接用于特征选择其实不是太方便：</p><ol type="1"><li>它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较</li><li>对于连续变量的计算不是很方便（ X 和 Y 都是集合, <spanclass="math inline">\(x_i\)</span>, <spanclass="math inline">\(y\)</span>都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感</li></ol><p><strong>最大信息系数</strong>克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0,1] 。<ahref="https://link.zhihu.com/?target=https%3A//minepy.readthedocs.io/en/latest/">minepy</a>提供了MIC功能。</p><p>下面我们来看下 <span class="math inline">\(y=x^2\)</span>这个例子，MIC算出来的互信息值为1(最大的取值)。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> minepy <span class="hljs-keyword">import</span> MINE<br><br>m = MINE()<br>x = np.random.uniform(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10000</span>)<br>m.compute_score(x, x**<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(m.mic())<br><br><br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-comment">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mic</span>(<span class="hljs-params">x, y</span>):<br>    m = MINE()<br>    m.compute_score(x, y)<br>    <span class="hljs-keyword">return</span> (m.mic(), <span class="hljs-number">0.5</span>)<br><span class="hljs-comment"># 选择K个最好的特征，返回特征选择后的数据</span><br>SelectKBest(<span class="hljs-keyword">lambda</span> X, Y: array(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><h3 id="距离相关系数">距离相关系数</h3><p>距离相关系数是为了克服Pearson相关系数的弱点而生的。在 <spanclass="math inline">\(x\)</span> 和 <spanclass="math inline">\(x^2\)</span> 这个例子中，即便Pearson相关系数是 0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0 ，那么我们就可以说这两个变量是独立的。</p><p>R的<ahref="https://link.zhihu.com/?target=https%3A//cran.r-project.org/web/packages/energy/index.html">energy</a>包里提供了距离相关系数的实现，另外这是<ahref="https://link.zhihu.com/?target=https%3A//gist.github.com/josef-pkt/2938402">Pythongist</a>的实现。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text"># R-code<br>&gt; x = runif (1000, -1, 1)<br>&gt; dcor(x, x**2)<br>[1] 0.4943864<br></code></pre></td></tr></table></figure><p>尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。</p><p>第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。</p><p>第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</p><h3 id="方差选择法">方差选择法</h3><p>过滤特征选择法还有一种方法不需要度量特征 <spanclass="math inline">\(x_i\)</span> 和类别标签 <spanclass="math inline">\(y\)</span>的信息量。这种方法先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p><p>例如，假设我们有一个具有布尔特征的数据集，并且我们要删除超过80％的样本中的一个或零（开或关）的所有特征。布尔特征是伯努利随机变量，这些变量的方差由下式给出:<span class="math inline">\(Var[X]=p(1−p)\)</span></p><p><ahref="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html%23sklearn.feature_selection.VarianceThreshold">VarianceThreshold</a>是特征选择的简单基线方法。它删除方差不符合某个阈值的所有特征。默认情况下，它会删除所有零差异特征，即所有样本中具有相同值的特征。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br>X = [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]<br><span class="hljs-comment"># 方差选择法，返回值为特征选择后的数据</span><br><span class="hljs-comment"># 参数threshold为方差的阈值</span><br>sel = VarianceThreshold(threshold=(<span class="hljs-number">.8</span> * (<span class="hljs-number">1</span> - <span class="hljs-number">.8</span>)))<br><span class="hljs-built_in">print</span>(sel.fit_transform(X))<br><br><span class="hljs-comment"># VarianceThreshold(threshold=3).fit_transform(iris.data)</span><br></code></pre></td></tr></table></figure><p>输出结果：</p><p>array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]])如预期的那样，VarianceThreshold已经删除了第一列，其具有 <spanclass="math inline">\(p=5/6&gt;0.8\)</span> 包含零的概率。</p><p>方差选择的逻辑并不是很合理，这个是基于各特征分布较为接近的时候，才能以方差的逻辑来衡量信息量。但是如果是离散的或是仅集中在几个数值上，如果分布过于集中，其信息量则较小。而对于连续变量，由于阈值可以连续变化，所以信息量不随方差而变。实际使用时，可以结合cross-validate进行检验</p><h2 id="包装法">包装法</h2><p>基本思想：基于hold-out方法，对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集。需要先选定特定算法，通常选用普遍效果较好的算法，例如Random Forest， SVM， kNN等等。</p><blockquote><p>西瓜书上说包装法应该欲训练什么算法，就选择该算法进行评估随着学习器（评估器）的改变，最佳特征组合可能会改变</p></blockquote><p>贪婪搜索算法（greedy search）是局部最优算法。与之对应的是穷举算法(exhaustivesearch)，穷举算法是遍历所有可能的组合达到全局最优级，但是计算复杂度是2^n，一般是不太实际的算法。</p><h3 id="前向搜索">前向搜索</h3><p>前向搜索说白了就是，每次增量地从剩余未选中的特征选出一个加入特征集中，待达到阈值或者n 时，从所有的 F 中选出错误率最小的。过程如下：</p><ol type="1"><li>初始化特征集 F 为空。</li><li>扫描 i 从 1 到 n 如果第 i 个特征不在 <spanclass="math inline">\(F\)</span> 中，那么特征 i 和F 放在一起作为 <spanclass="math inline">\(F_i\)</span> (即$ F_i=F )$。 在只使用 <spanclass="math inline">\(F_i\)</span>中特征的情况下，利用交叉验证来得到 F_i的错误率。</li><li>从上步中得到的 n 个 <span class="math inline">\(F_i\)</span>中选出错误率最小的 <span class="math inline">\(F_i\)</span> ,更新 F 为<span class="math inline">\(F_i\)</span>。</li><li>如果 F 中的特征数达到了 n 或者预定的阈值（如果有的话），那么输出整个搜索过程中最好的 ；若没达到，则转到 2，继续扫描。</li></ol><h3 id="后向搜索">后向搜索</h3><p>既然有增量加，那么也会有增量减，后者称为后向搜索。先将 F 设置为 <spanclass="math inline">\({1,2,...,n}\)</span>，然后每次删除一个特征，并评价，直到达到阈值或者为空，然后选择最佳的 F。</p><p>这两种算法都可以工作，但是计算复杂度比较大。时间复杂度为：O(n+(n−1)+(n−2)+...+1)=O(n2)</p><h3 id="递归特征消除法">递归特征消除法</h3><p>递归消除特征法使用一个<code>基模型</code>来进行多轮训练，每轮训练后通过学习器返回的coef_ 或者feature_importances_消除若干权重较低的特征，再基于新的特征集进行下一轮训练。</p><p>使用feature_selection库的RFE类来选择特征的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br><span class="hljs-comment">#递归特征消除法，返回特征选择后的数据</span><br><span class="hljs-comment">#参数estimator为基模型</span><br><span class="hljs-comment">#参数n_features_to_select为选择的特征个数</span><br>RFE(estimator=LogisticRegression(), n_features_to_select=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><h2 id="嵌入法">嵌入法</h2><ul><li>基于惩罚项的特征选择法通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br><span class="hljs-comment">#带L1惩罚项的逻辑回归作为基模型的特征选择   </span><br>SelectFromModel(LogisticRegression(penalty=<span class="hljs-string">&quot;l1&quot;</span>, C=<span class="hljs-number">0.1</span>)).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><p>要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><br><span class="hljs-comment">#带L1和L2惩罚项的逻辑回归作为基模型的特征选择   </span><br><span class="hljs-comment">#参数threshold为权值系数之差的阈值   </span><br>SelectFromModel(LR(threshold=<span class="hljs-number">0.5</span>, C=<span class="hljs-number">0.1</span>)).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><ul><li>基于学习模型的特征排序这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。通过这种训练对特征进行打分获得相关性后再训练最终模型。</li></ul><p>在<ahref="https://link.zhihu.com/?target=https%3A//archive.ics.uci.edu/ml/datasets/Housing">波士顿房价数据集</a>上使用sklearn的<ahref="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">随机森林回归</a>给出一个<strong>单变量选择</strong>的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> cross_val_score, ShuffleSplit<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_boston<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor<br><br><span class="hljs-comment">#加载波士顿房价作为数据集</span><br>boston = load_boston()<br>X = boston[<span class="hljs-string">&quot;data&quot;</span>]<br>Y = boston[<span class="hljs-string">&quot;target&quot;</span>]<br>names = boston[<span class="hljs-string">&quot;feature_names&quot;</span>]<br><br><span class="hljs-comment">#n_estimators为森林中树木数量，max_depth树的最大深度</span><br>rf = RandomForestRegressor(n_estimators=<span class="hljs-number">20</span>, max_depth=<span class="hljs-number">4</span>)<br>scores = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>    <span class="hljs-comment">#每次选择一个特征，进行交叉验证，训练集和测试集为7:3的比例进行分配，</span><br>    <span class="hljs-comment">#ShuffleSplit()函数用于随机抽样（数据集总数，迭代次数，test所占比例）</span><br>    score = cross_val_score(rf, X[:, i:i+<span class="hljs-number">1</span>], Y, scoring=<span class="hljs-string">&quot;r2&quot;</span>,<br>                               cv=ShuffleSplit(<span class="hljs-built_in">len</span>(X), <span class="hljs-number">3</span>, <span class="hljs-number">.3</span>))<br>    scores.append((<span class="hljs-built_in">round</span>(np.mean(score), <span class="hljs-number">3</span>), names[i]))<br><br><span class="hljs-comment">#打印出各个特征所对应的得分</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">sorted</span>(scores, reverse=<span class="hljs-literal">True</span>))<br></code></pre></td></tr></table></figure><p>输出结果：</p><p>[(0.64300000000000002, 'LSTAT'), (0.625, 'RM'), (0.46200000000000002,'NOX'), (0.373, 'INDUS'), (0.30299999999999999, 'TAX'),(0.29799999999999999, 'PTRATIO'), (0.20399999999999999, 'RAD'), (0.159,'CRIM'), (0.14499999999999999, 'AGE'), (0.097000000000000003, 'B'),(0.079000000000000001, 'ZN'), (0.019, 'CHAS'), (0.017999999999999999,'DIS')]</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征选择（Feature selection）</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#4 逻辑回归：二分类到多分类</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB.html</url>
    
    <content type="html"><![CDATA[<p>前文介绍了基本的线性回归问题，尝试预测一系列<strong>连续值</strong>属性。现在我们将介绍的分类问题则关注<strong>离散值</strong>属性的预测。</p><p>在分类问题中，我们尝试预测的结果是否属于某一个类，最基础的就是<strong>二元</strong>的分类问题（例如预测垃圾邮件、恶性肿瘤），更为复杂的则是预测<strong>多元</strong>的分类问题。</p><h2 id="二分类问题">二分类问题</h2><p>分类问题的样本与回归问题类似，由特征和目标构成，给定数据集：</p><p><spanclass="math display">\[\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span></p><ul><li><span class="math inline">\(x(i)\)</span> 代表第 <spanclass="math inline">\(i\)</span> 个观察实例的 <spanclass="math inline">\(n+1\)</span> 维<strong>特征向量</strong> <spanclass="math inline">\(\left(x_0^{(i)},\cdots,x_n^{(i)}\right)^T\)</span>；</li><li><span class="math inline">\(y^{(i)}\in\{0,1\}\)</span> 代表第 <spanclass="math inline">\(i\)</span>个观察实例的<strong>目标变量</strong>，在这里有 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span> 两类结果。</li></ul><p>即对于输入的自变量 <span class="math inline">\(x^(i)\)</span>，因变量<span class="math inline">\(y^(i)\)</span> 可能为 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span>。其中 <spanclass="math inline">\(0\)</span> 表示<strong>负向类</strong>（negativeclass），<span class="math inline">\(1\)</span>表示<strong>正向类</strong>（positive class）。</p><blockquote><p>我们不对「正向」和「负向」加以特殊区分，但在实际应用中「正向」通常表示「具有我们要寻找的东西」，如垃圾邮件、恶性肿瘤等。</p></blockquote><h3 id="线性回归的不足">线性回归的不足</h3><p>首先可能会自然而然地想到用之前的线性回归来解决——用一条直线拟合结果，当预测值大于0.5 时归为正向类，反之归为负向类。</p><p>这看似合理，然而，线性回归保留了 <spanclass="math inline">\(y^(i)\)</span>太多的「<strong>信息量</strong>」。对于某些「<strong>反常样本</strong>」，我们可能预测出一个远大于<span class="math inline">\(1\)</span> 或者远小于 <spanclass="math inline">\(0\)</span>的结果，同理，这些「反常样本」用于拟合直线时也会对其造成一定偏移，以至于正常样本被归为错误类别。</p><figure><img src="http://img.hexiaobo.xyz/linear-regression.png"alt="反常样本使得蓝线偏移" /><figcaption aria-hidden="true">反常样本使得蓝线偏移</figcaption></figure><h2 id="逻辑回归-logistic-regression">逻辑回归 | LogisticRegression</h2><p>线性回归和逻辑回归都属于<strong>广义线性模型</strong>（GeneralizedLinearModel）的特殊形式，线性模型都可用于回归问题的求解。但由于<strong>逻辑函数</strong>（LosisticFunction）将结果映射到 <strong>Bernoulli分布</strong>，因此逻辑回归更常用于分类问题。</p><h3 id="假说表示-hypothesis-representation">假说表示 | HypothesisRepresentation</h3><p>回忆线性回归的假设函数：<spanclass="math inline">\(h_\theta(x)=\theta^Tx\)</span>，我们在其外套上<span class="math inline">\(sigmoid\)</span>函数，构造逻辑回归的假设函数为：</p><p><spanclass="math display">\[h_\theta(x)=g\left(\theta^Tx\right)=\frac{1}{1+e^{-\theta^Tx}}\]</span></p><blockquote><p>所谓 sigmoid 函数（也即前面提到的<strong>逻辑函数</strong>）：</p><p><span class="math display">\[g(z)=\frac{1}{1+e^{-z}}\]</span></p><figure><img src="http://img.hexiaobo.xyz/sigmoid.png" alt="sigmoid函数" /><figcaption aria-hidden="true">sigmoid函数</figcaption></figure><p>是一个介于 <span class="math inline">\((0,1)\)</span>之间的单增 <spanclass="math inline">\(S\)</span> 形函数，其导出需要用到 GLMs和指数分布族（The Exponential Family）的知识。</p></blockquote><p>也就是说，对于一个参数为 <span class="math inline">\(θ\)</span>的逻辑回归模型，输入 <span class="math inline">\(x\)</span>，得到 <spanclass="math inline">\(h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\)</span>的预测值。</p><p>我们可以把这个输出值视为 <span class="math inline">\(x\)</span>这个样本对应的 <span class="math inline">\(y\)</span> 等于 1的<strong>概率</strong>（estimated probablity），即 <spanclass="math inline">\(h_\theta \left( x \right)=P\left( y=1|x;\theta\right)\)</span>。针对分类情形，我们可以认为如果概率 <spanclass="math inline">\(\geqslant 0.5\)</span>，则分类为 <spanclass="math inline">\(1\)</span>，否则分类为 <spanclass="math inline">\(0\)</span>。</p><h3 id="决策边界-decision-boundary">决策边界 | Decision Boundary</h3><p>又根据 <span class="math inline">\(sigmoid\)</span> 函数的性质:</p><p><span class="math display">\[h_\theta(x)\geqslant 0.5\iff\theta^Tx\geqslant0\]</span></p><p>所以只要 <spanclass="math inline">\(\theta^Tx\geqslant0\)</span>，就会分类为 <spanclass="math inline">\(1\)</span>，否则分类为 <spanclass="math inline">\(0\)</span>；于是乎，<spanclass="math inline">\(\theta^Tx=0\)</span>解出的这条「<strong>线</strong>」（对于高维情形为<strong>超平面</strong>）被称作决策边界，它将整个空间划分成两块区域（region），各自属于一个分类。</p><p>下面看两个二维情形的例子：</p><figure><img src="http://img.hexiaobo.xyz/decision-boundary-1.png"alt="线性的决策边界" /><figcaption aria-hidden="true">线性的决策边界</figcaption></figure><p>对于上述样本点的分布，用一条直线即可划分空间，对应的假设函数为 <spanclass="math inline">\(h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2x_2\right)\)</span>。</p><figure><img src="http://img.hexiaobo.xyz/decision-boundary-2.png"alt="多项式的决策边界" /><figcaption aria-hidden="true">多项式的决策边界</figcaption></figure><p>而对于这种分布，我们必须选择二维曲线来划分空间，即使用<strong>多项式特征</strong>来确定曲线的参数，对应的假设函数为<span class="math inline">\(h_\theta(x)=g\left(\theta_0+\theta_1x_1+\theta_2 x_2+\theta_3 x_3^2+\theta_4x_4^2\right)\)</span>。当然，我们也可以用更复杂的多项式曲线来划分更复杂的分布。</p><h3 id="代价函数">代价函数</h3><p>现在，我们的任务就是从训练集中拟合逻辑回归的参数 <spanclass="math inline">\(θ\)</span>。仍然采用代价函数的思想——找到使代价最小的参数即可。</p><p>广义上来讲，代价函数是这样的一个函数：</p><p><spanclass="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\]</span></p><p>也就是说用每个数据的估计值 <spanclass="math inline">\(h_\theta(x^{(i)})\)</span> 和真实值 <spanclass="math inline">\(y^(i)\)</span> 计算一个代价 <spanclass="math inline">\(\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\)</span>，比如线性回归中这个代价就是二者差值的平方。</p><p>理论上来说，我们也可以对逻辑回归模型沿用平方误差的定义，但当我们将<span class="math inline">\({h_\theta}\left( x\right)=\frac{1}{1+{e^{-\theta^{T}x}}}\)</span>代入到这样的代价函数中时，我们得到的将是一个<strong>非凸函数</strong>（non-convexfunction）。这意味着空间中会有许多<strong>局部最小值</strong>，使得梯度下降法难以寻找到<strong>全局最小值</strong>。</p><p>因此我们重新定义逻辑回归的<strong>代价函数</strong>：</p><p><span class="math display">\[\mathrm{Cost}\left( h_{\theta}(x),y\right) =\begin{cases}    -\ln \left( h_{\theta}\left( x \right) \right)&amp;        y=1\\    -\ln \left( 1-h_{\theta}\left( x \right) \right)&amp;        y=0\\\end{cases}\]</span></p><p>绘制出的曲线大致呈这样：</p><figure><img src="http://img.hexiaobo.xyz/cost.png" alt="代价函数" /><figcaption aria-hidden="true">代价函数</figcaption></figure><p>观察曲线，发现当 <spanclass="math inline">\(y=1\)</span>（样本的真实值为 <spanclass="math inline">\(1\)</span>）时，预测值 <spanclass="math inline">\(h_\theta(x)\)</span> 越接近 <spanclass="math inline">\(1\)</span> 则代价越小，越接近 <spanclass="math inline">\(0\)</span>则代价趋于无穷。譬如在肿瘤分类中，将实际为恶性的肿瘤以百分之百的概率预测为良性，带来的后果将不可估量。</p><p>与此同时，注意到代价函数也可以<strong>简写</strong>为：</p><p><span class="math display">\[\mathrm{Cost}\left( h_{\theta}\left( x\right) ,y \right) =-\left[ y\ln \left( h_{\theta}\left( x \right)\right) +\left( 1-y \right) \ln \left( 1-h_{\theta}\left( x \right)\right) \right]\]</span></p><p>它还有另外一个名称——<strong>二元交叉熵代价函数</strong>（BCE, BinaryCross-Entropy），它又蕴含着怎样的原理呢？</p><h3 id="代价函数的数学推导">代价函数的数学推导</h3><p>首先明确什么是一个<strong>好的代价函数</strong>——当参数 <spanclass="math inline">\(θ\)</span> 使得 <spanclass="math inline">\(J(θ)\)</span> 取<strong>极小值</strong>时，这个<span class="math inline">\(θ\)</span>也能使模型拟合效果最好。这时我们回忆起 <strong>极大似然估计</strong>的思想：当参数 <span class="math inline">\(θ\)</span> 使得 <spanclass="math inline">\(L(θ)\)</span> 取<strong>极大值</strong>时，这个<span class="math inline">\(θ\)</span>也能使得<strong>事件组</strong>最容易发生！</p><p>前文已经提到，我们用概率解释预测值 <spanclass="math inline">\(h_\theta(x)=P(y=1)\)</span>，于是 <spanclass="math inline">\(1-h_\theta(x)=P(y=0)\)</span>，故：</p><p><span class="math display">\[P\left( y=k \right) =\left[h_{\theta}\left( x \right) \right] ^k\left[ 1-h_{\theta}\left( x \right)\right] ^{1-k},\quad k\in \left\{ 0,1 \right\}\]</span></p><p>而对于数据集 <spanclass="math inline">\(\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\)</span>下，将其视为已发生的一个<strong>事件组</strong>，则似然函数为：</p><p><span class="math display">\[L\left( \theta \right)=\prod_{i=1}^m{P}\left( y=y^{(i)} \right) =\prod_{i=1}^m{\left[h_{\theta}\left( x^{(i)} \right) \right] ^{y^{(i)}}}\left[1-h_{\theta}\left( x^{(i)} \right) \right] ^{1-y^{(i)}}\]</span></p><p>取对数得到：</p><p><span class="math display">\[\ln L(\theta )=\sum_{i=1}^m{\left\{y^{(i)}\ln \left[ h_{\theta}\left( x^{(i)} \right) \right] +\left(1-y^{(i)} \right) \ln \left[ 1-h_{\theta}\left( x^{(i)} \right) \right]\right\}}\]</span></p><p>注意到，极大似然法的目标是找到 <spanclass="math inline">\(L(θ)\)</span> 或 <span class="math inline">\(\lnL(\theta)\)</span> 的极大值，而逻辑回归的目标是找到 <spanclass="math inline">\(J(θ)\)</span> 的极小值，所以自然的，我们将 <spanclass="math inline">\(ln⁡ L(θ)\)</span> <strong>取反</strong>来定义 <spanclass="math inline">\(J(θ)\)</span>：</p><p><span class="math display">\[\begin{aligned}J(\theta)&amp;=-\frac{1}{m}\ln L(\theta)\\&amp;=-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left(i\right)}\ln \left(h_{\theta}\left( x^{\left(i\right)} \right) \right) +\left(1-y^{\left(i\right)} \right) \ln \left( 1-h_{\theta}\left(x^{\left(i\right)} \right) \right) \right]}\end{aligned}\]</span></p><p>其中 <span class="math inline">\(\frac{1}{m}\)</span> 对要求的 <spanclass="math inline">\(\theta\)</span> 没有影响，仅是取一下平均罢了。</p><blockquote><p>可以证明上述代价函数 <span class="math inline">\(J(θ)\)</span>会是一个<strong>凸函数</strong>，并且没有局部最优值。凸性分析的内容不在本讲的范围，但是可以证明我们所选的代价函数会给我们带来一个<strong>凸优化</strong>问题（ConvexOptimization）。</p></blockquote><h3 id="梯度下降">梯度下降</h3><p>既然是凸函数，那么现在我们就可以进行梯度下降求解 <spanclass="math inline">\(\underset{\theta}{\arg\min }J\left( \theta\right)\)</span> 。</p><p>为了求偏导，我们先计算：</p><p><span class="math display">\[\begin{aligned}    \frac{\partial}{\partial \theta}\mathrm{Cost}\left( h_{\theta}\left(x \right) ,y \right) &amp;=\frac{\partial}{\partial \theta}\left[ -y\ln\left( h_{\theta}\left( x \right) \right) -\left( 1-y \right) \ln \left(1-h_{\theta}\left( x \right) \right) \right]\\    &amp;=\frac{\partial}{\partial \theta}\left[ y\ln \left(1+e^{-\theta ^Tx} \right) +(1-y)\ln \left( 1+e^{\theta ^Tx} \right)\right]\\    &amp;=\frac{-yxe^{-\theta ^Tx}}{1+e^{-\theta ^Tx}}+\frac{\left( 1-y\right) xe^{\theta ^Tx}}{1+e^{\theta ^Tx}}\\    &amp;=\frac{-yx+\left( 1-y \right) xe^{\theta ^Tx}}{1+e^{\theta^Tx}}\\    &amp;=\left( -y+\frac{1}{1+e^{-\theta ^Tx}} \right) x\\    &amp;=\left( h_{\theta}\left( x \right) -y \right) x\\\end{aligned}\]</span></p><p>于是乎，</p><p><span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i\right)} \right) -y^{\left( i \right)} \right) x^{\left( i\right)}}\]</span></p><p>没错，这个偏导的形式和线性回归完全相同！不同的只是 <spanclass="math inline">\({h_\theta}\left( x \right)=g\left( {\theta^T}X\right)\)</span> 的定义——多了一层 <spanclass="math inline">\(sigmoid\)</span>函数，正是因此，我们不能使用正规方程直接给出<strong>解析解</strong>，而必须使用梯度下降等方法。</p><p><span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partialJ}{\partial \theta}\]</span></p><p>现在我们对其使用梯度下降即可。另外，在运行梯度下降算法之前，进行<strong>特征缩放</strong>依旧是非常必要的。</p><blockquote><p>除了梯度下降法，还有很多算法可以用来求解这个最优值：共轭梯度法（ConjugateGradient）、局部优化法（Broyden fletcher goldfarb shann,BFGS）、有限内存局部优化法（LBFGS）等。</p><p>这些算法通常不需要手动选择学习率 <spanclass="math inline">\(α\)</span>，而是使用一个智能的内循环（线性搜索算法）来选择一个较好的<span class="math inline">\(α\)</span>，甚至能为每次迭代选择不同的 <spanclass="math inline">\(α\)</span>。因此他们有着更优越的常数和时间复杂度，在大型机器学习项目中更加适用。</p></blockquote><h3 id="代码实现">代码实现</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a>上的二分类数据集 <code>ex2data1.txt</code>为例，首先看一下数据的分布：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-meta"># load data, data.shape = (100, 3)</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = np.loadtxt(&#x27;<span class="hljs-title">ex2data1</span>.<span class="hljs-title">txt&#x27;</span>, <span class="hljs-title">delimiter</span>=&#x27;,&#x27;)</span><br>(m, n) = <span class="hljs-class"><span class="hljs-keyword">data</span>.shape</span><br><span class="hljs-type">X</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>[:, :-1]</span><br><span class="hljs-title">y</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>[:, -1]</span><br><br><span class="hljs-meta"># preview data</span><br><span class="hljs-title">pos</span> = np.<span class="hljs-keyword">where</span>(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-title">neg</span> = np.<span class="hljs-keyword">where</span>(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]  # 返回索引<br><span class="hljs-title">plt</span>.scatter(<span class="hljs-type">X</span>[pos, <span class="hljs-number">0</span>], <span class="hljs-type">X</span>[pos, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=&#x27;c&#x27;)<br><span class="hljs-title">plt</span>.scatter(<span class="hljs-type">X</span>[neg, <span class="hljs-number">0</span>], <span class="hljs-type">X</span>[neg, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=&#x27;r&#x27;)<br><span class="hljs-title">plt</span>.xlabel(&#x27;<span class="hljs-type">Exam</span> <span class="hljs-number">1</span> score&#x27;)<br><span class="hljs-title">plt</span>.ylabel(&#x27;<span class="hljs-type">Exam</span> <span class="hljs-number">2</span> score&#x27;)<br><span class="hljs-title">plt</span>.show()<br><br><span class="hljs-type">PYTHON</span><br></code></pre></td></tr></table></figure><figure><img src="http://img.hexiaobo.xyz/preview_scatter.png"alt="数据分布散点图" /><figcaption aria-hidden="true">数据分布散点图</figcaption></figure><p>看起来用<strong>直线</strong>即可划分数据。此外，注意到如果每次都用<code>np.sum()</code> 计算 <spanclass="math inline">\(\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j\)</span>耗时较大，因此将求和化成<strong>矩阵形式</strong>：</p><p><span class="math display">\[\theta :=\theta -\alpha\frac{1}{m}X^T\left( g\left( X\theta \right) -y \right)\]</span></p><p>实现逻辑回归如下，矩阵化后运行时间可缩短一半：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import matplotlib.pyplot as plt<br><br><span class="hljs-comment"># load data, data.shape = (100, 3)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex2data1.txt&#x27;</span>, <span class="hljs-attribute">delimiter</span>=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-1]<br>y = data[:, -1]<br><br><span class="hljs-comment"># normalization</span><br>X = (X - X.mean(<span class="hljs-attribute">axis</span>=0)) / X.std(<span class="hljs-attribute">axis</span>=0, <span class="hljs-attribute">ddof</span>=1)<br>X = np.c_[np.ones(m), X] # 增加一列 1<br><br><span class="hljs-comment"># parameters</span><br>alpha = 0.01<br>num_iters = 10000<br>theta = np.zeros(n)<br><br>def sigmoid(z):<br>    g = np.zeros(z.size)<br>    g = 1 / (1 + np.exp(-z))<br>    return g<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(0, num_iters):<br><span class="hljs-built_in">error</span> = sigmoid(X @ theta) - y  # error.shape = (100, )<br>theta -= (alpha / m) * (X.T @ error)  # X.T.shape = (2, 100)<br><br><span class="hljs-comment"># plot decision boundary</span><br>pos = np.where(y == 1)[0]<br>neg = np.where(y == 0)[0]<br>plt.scatter(X[pos, 1], X[pos, 2], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;o&quot;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;c&#x27;</span>)<br>plt.scatter(X[neg, 1], X[neg, 2], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;r&#x27;</span>)<br><br>x_plot = np.array([np.min(X[:, 1]), np.max(X[:, 1])])<br>y_plot = (-1 / theta[2]) * (theta[1] * x_plot + theta[0])<br>plt.plot(x_plot, y_plot)<br>plt.show()<br><br>PYTHON<br></code></pre></td></tr></table></figure><p>得到的 <span class="math inline">\(\left( \theta_0, \theta_1,\theta_2 \right)\)</span> 结果是：[1.2677 3.05552.8289]，绘制出决策边界的图像为：</p><figure><img src="http://img.hexiaobo.xyz/decision_boundary.png"alt="决策边界（归一化）" /><figcaption aria-hidden="true">决策边界（归一化）</figcaption></figure><h2 id="多分类问题">多分类问题</h2><p>在实际情形中，我们还会使用逻辑回归来解决<strong>多元</strong>的分类问题。多分类的数据集和二分类相似，区别在于<strong>目标变量</strong><span class="math inline">\(y^(i)\)</span> 在这里不仅有 <spanclass="math inline">\(0\)</span> 或 <spanclass="math inline">\(1\)</span> 两类结果，还可以取 <spanclass="math inline">\(2、3\)</span> 等更多的数字。</p><figure><img src="http://img.hexiaobo.xyz/multi-class.png"alt="二分类和多分类" /><figcaption aria-hidden="true">二分类和多分类</figcaption></figure><blockquote><p>对于接下来要介绍的方法，标签数字的顺序、取法，都不会影响最终的结果。但在某些分类模型中，数值可能具有实际意义，这时候使用<strong>独热码</strong>（One-Hot）或许是更好的选择。</p></blockquote><h3 id="一对余-one-vs.-rest">一对余 | One vs. Rest</h3><p>对于 <span class="math inline">\(N\)</span>分类问题，我们可以将其转化为 <span class="math inline">\(N\)</span>个二分类问题——只需创建 <span class="math inline">\(N\)</span>个「<strong>伪训练集</strong>」，每个训练集中仅包含一个类作为正向类，其他<span class="math inline">\(N−1\)</span> 个类均视为负向类。</p><figure><img src="http://img.hexiaobo.xyz/one-vs-rest.png" alt="One vs. Rest" /><figcaption aria-hidden="true">One vs. Rest</figcaption></figure><p>接下来就可以训练 <span class="math inline">\(N\)</span>个标准的逻辑回归分类器，将其记为：</p><p><span class="math display">\[h_\theta^{\left( i \right)}\left( x\right)=P\left( y=i|x;\theta  \right) \;\; i=\left( 1,2,\cdots,N\right)\]</span></p><p>显然，每个分类器的输出都可以视为「<strong>属于某类</strong>」的概率，在预测时，我们只需要运行一遍所有分类器，然后取其最大值作为预测结果即可。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#3 正规方程：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>前文提到线性回归问题时，我们说在数学上也可以用最小二乘法（LeastSquareMethod）来解决，实际上其思路也是<strong>最小化</strong>平方误差代价函数（也称<strong>残差函数</strong>）。</p><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，最小二乘法的矩阵解法——正规方程则是更好的解决方案。</p><h2 id="正规方程-normal-equation">正规方程 | Normal Equation</h2><p>利用多元微分学知识，我们知道对于代价函数：</p><p><span class="math display">\[J(\theta)=J(\theta_0, \theta_1,\cdots,\theta_n)\]</span></p><p>如果它是<strong>连续</strong>的，则要求出它的最小值，只需要令各偏导为零：</p><p><span class="math display">\[\frac{\partial J}{\partial\theta_j}=0,\quad j=0,1,\cdots,n\]</span></p><p>或写作向量形式：</p><p><span class="math display">\[\frac{\partial J}{\partial \theta}=\vec0\]</span></p><p>就能解出令 <span class="math inline">\(J(\theta)\)</span> 最小化的<span class="math inline">\(θ\)</span> 值。</p><p>由此，我们将代价函数转化为<strong>有确定解的代数方程组</strong>（其方程式数目正好等于未知数的个数），这个方程组就是正规方程（NormalEquation）。</p><h3 id="数学推导-1">数学推导 1</h3><p>下面我们就对多元线性回归的代价函数进行求解：</p><p><spanclass="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span></p><p>于是其偏导函数为：</p><p><span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span></p><p>要使之为<strong>零向量</strong>，只能是：</p><p><span class="math display">\[\theta^Tx^{(i)}=y^{(i)},\quadi=1,2,\cdots,m\]</span></p><p>恒成立。写作矩阵为：</p><p><span class="math display">\[\theta ^TX^T=y^T\text{ ，或 }X\theta=y\]</span></p><p>其中，</p><p><span class="math display">\[X_{(n+1)\times m}=\left[ \begin{matrix}    x_{0}^{(1)}&amp;        x_{1}^{(1)}&amp;        \cdots&amp;        x_{n}^{(1)}\\    x_{0}^{(2)}&amp;        x_{1}^{(2)}&amp;        \cdots&amp;        x_{n}^{(2)}\\    \vdots&amp;        \vdots&amp;        \ddots&amp;        \vdots\\    x_{0}^{(m)}&amp;        x_{1}^{(m)}&amp;        \cdots&amp;        x_{n}^{(m)}\\\end{matrix} \right] =\left[ \begin{array}{c}    {x^{(1)}}^T\\    {x^{(2)}}^T\\    \vdots\\    {x^{(m)}}^T\\\end{array} \right] ,\quad y=\left[ \begin{array}{c}    y^{(1)}\\    y^{(2)}\\    \vdots\\    y^{(m)}\\\end{array} \right]\]</span></p><p>两边同时乘以 <span class="math inline">\(X^T\)</span>，假设 <spanclass="math inline">\(X^T\)</span>可逆，解得：</p><p><span class="math display">\[\theta=(X^TX)^{-1}X^Ty\]</span></p><h3 id="数学推导-2">数学推导 2</h3><p>前面的推导中，在<strong>向量形式</strong>的偏导函数中发现了简化条件，将零向量提出来单独求解。下面介绍另一个<strong>纯矩阵形式</strong>的解法。</p><p>首先将代价函数表示为：</p><p><span class="math display">\[\begin{aligned}    J(\theta )&amp;=\frac{1}{2m}\left( X\theta -y \right) ^T\left(X\theta -y \right)\\    &amp;=\frac{1}{2}\left( \theta ^TX^T-y^T \right) \left( X\theta -y\right)\\    &amp;=\frac{1}{2}\left( \theta ^TX^TX\theta -\theta^TX^Ty-y^TX\theta +y^Ty \right)\\\end{aligned}\]</span></p><p>接下来对 J(θ)求偏导，需要用到<strong>矩阵的求导法则</strong>（证明过程略去不表）：</p><blockquote><ol type="1"><li><p>当 <span class="math inline">\(f(x) = Ax\)</span> 时， <spanclass="math display">\[\frac{\partial f (x)}{\partial x^T}  = \frac{\partial (Ax)}{\partialx^T}  =A\]</span></p></li><li><p>当<span class="math inline">\(f(x) = x^TAx\)</span> 时，</p><p><span class="math display">\[\frac{\partial f (x)}{\partial x}  =\frac{\partial (x^TAx)}{\partial x}  =Ax+A^Tx\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = a^Tx\)</span> 时，</p><p><span class="math display">\[\frac{\partial a^Tx}{\partial x}  =\frac{\partial x^Ta}{\partial x}  =a\]</span></p></li><li><p>当 <span class="math inline">\(f(x) = x^TAy\)</span>时，</p><p><span class="math display">\[\frac{\partial x^TAy}{\partial x}  =Ay\]</span></p></li></ol></blockquote><p>分别用法则 2、4、3 求导，得到：</p><p><span class="math display">\[\begin{aligned}    \frac{\partial J\left( \theta \right)}{\partial\theta}&amp;=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-(y^TX)^T+0 \right)\\    &amp;=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-X^Ty+0 \right)\\    &amp;=\frac{1}{m}\left(X^TX\theta -X^Ty\right)\\\end{aligned}\]</span></p><p>令偏导为零，解得：</p><p><span class="math display">\[\theta =\left( X^TX \right)^{-1}X^Ty\]</span></p><h3 id="梯度下降-vs.-正规方程">梯度下降 vs. 正规方程</h3><p>观察到在正规方程的结果中，<span class="math inline">\(X^TX\)</span>是一个 <span class="math inline">\((n+1)\times(n+1)\)</span>的矩阵，因此直接取逆计算 <span class="math inline">\(θ\)</span>的复杂度是 <span class="math inline">\(O(n^3)\)</span> 。如果 <spanclass="math inline">\(n\)</span> 不是很大，这是有效的，但是如果 <spanclass="math inline">\(n\)</span> 达到了 <spanclass="math inline">\(10^4,10^5\)</span>或更高，就需要使用梯度下降了。</p><p>下面从其他方面对两种算法进行比较：</p><table><thead><tr class="header"><th style="text-align: center;">区别</th><th style="text-align: center;">梯度下降</th><th style="text-align: center;">正规方程</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">学习率 α</td><td style="text-align: center;">需要选择</td><td style="text-align: center;">不需要</td></tr><tr class="even"><td style="text-align: center;">迭代</td><td style="text-align: center;">需要多次迭代</td><td style="text-align: center;">一次运算得出</td></tr><tr class="odd"><td style="text-align: center;">n 的取值</td><td style="text-align: center;">当 n 大时也能较好适用</td><td style="text-align: center;">当 n 小于时 104 还是可以接受的</td></tr><tr class="even"><td style="text-align: center;">特征缩放</td><td style="text-align: center;">特征取值范围相差大时需要</td><td style="text-align: center;">不需要缩放</td></tr><tr class="odd"><td style="text-align: center;">适用情形</td><td style="text-align: center;">适用于各种类型的模型</td><td style="text-align: center;">只适用于线性模型</td></tr></tbody></table><blockquote><p>这里提及适用情形，是因为随着问题的深入，算法将越发复杂。例如在分类算法中的逻辑回归等模型，就无法使用正规方程求解。</p></blockquote><h3 id="代码实现">代码实现</h3><p>下面仍以 <code>ex1data2.txt</code> 为例实现：</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stan">import numpy as np<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br><span class="hljs-title">data</span> = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=&#x27;,&#x27;)<br>(m, n) = <span class="hljs-title">data</span>.shape<br>X = np.c_[np.ones(m), <span class="hljs-title">data</span>[:, :-<span class="hljs-number">1</span>]]<br>y = <span class="hljs-title">data</span>[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Normal Equation</span><br>theta = np.linalg.<span class="hljs-built_in">inv</span>(X.T @ X) @ X.T @ y<br><span class="hljs-built_in">print</span>(theta)<br><br><span class="hljs-comment"># predict</span><br>predict = np.<span class="hljs-type">array</span>([<span class="hljs-number">1</span>, <span class="hljs-number">1650</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br>PYTHON<br></code></pre></td></tr></table></figure><p>很快就计算完了，预测在 <span class="math inline">\(\left(x_1=1650,x_2=3 \right)\)</span> 时的房价为293081.46433489426。大约相当于 3000 次梯度下降迭代的精度。</p><h3 id="不可逆情形">不可逆情形</h3><p>前一节的推导基于 <span class="math inline">\(X^TX\)</span>可逆（Invertible）的假设，如若不可逆（Non-invertible，也称Singular），我们只需将代码中的 <code>inv()</code> 换成<code>pinv()</code> 求出<strong>伪逆矩阵</strong>即可。</p><p>通常导致矩阵不可逆的原因可能有：</p><ul><li>存在冗余特征（特征之间不相互独立）；</li><li>特征数 <span class="math inline">\(n\)</span> 远大于样本数 <spanclass="math inline">\(m\)</span>（样本数不足以刻画这么多特征）。</li></ul><p>解决方法对应为：</p><ul><li>删除冗余特征（线性相关特征只保留其一）；</li><li>削减非必要的特征，或<strong>正则化</strong>方法（Regularization），后文将介绍。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#2 梯度下降：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>在前文 <ahref="http://hexiaobo.xyz/ML学习笔记-1-梯度下降：一元线性回归">一元线性回归</a>的基础上，我们引入多个特征变量，探讨梯度下降对多元线性回归的解法。此外，下一节将介绍正规方程在解多元线性回归中的应用。</p><h2 id="多元线性回归-multiple-linear-regression">多元线性回归 | MultipleLinear Regression</h2><p>现在我们的样本点 <span class="math inline">\(\left(x^{(i)},y^{(i)}\right)\)</span>有多个特征作为<strong>输入变量</strong>，即给定的数据集为： <spanclass="math display">\[\left\{\left(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span></p><ul><li><span class="math inline">\(n\)</span> 代表单个样本的特征数量；</li><li><span class="math inline">\({x}^{(i)}\)</span> 代表第 i个观察实例的<strong>特征向量</strong>；</li><li><span class="math inline">\(x^{(i)}_j\)</span> 代表第 i个观察实例的第$ j $个<strong>特征分量</strong>。</li></ul><p>同时，回归方程 h 也具有多个参数<spanclass="math inline">\(\theta_0,\theta_1,\cdots,\theta_n\)</span>： <spanclass="math display">\[h_\theta(x)=\theta_0+\theta_1x_1\cdots+\theta_nx_n\]</span> 为简化表达式，这里假定<span class="math inline">\(x_0 \equiv1\)</span> ，并以<strong>向量</strong>（vector）表示参数和自变量：<spanclass="math inline">\(\theta=(\theta_0,\cdots,\theta_n)^T,\;x=(x_0,\cdots,x_n)^T\)</span>，得到：<span class="math display">\[h_\theta(x)=\theta^Tx\]</span></p><h3 id="多变量梯度下降">多变量梯度下降</h3><p>类似地，我们定义平方误差代价函数： <span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span>我们的目标和一元线性回归中一样，要找出使得代价函数最小的一系列参数。于是，<span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 梯度下降时，不断作迭代： <span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}\]</span> 即可。</p><h3 id="特征缩放与标准化-standardization">特征缩放与标准化 |Standardization</h3><p>当不同自变量取值范围相差较大时，绘制的<strong>等高线图</strong>上的椭圆会变得瘦长，而梯度下降<strong>算法收敛</strong>将会很慢，因为每一步都可能会跨过这个椭圆导致<strong>振荡</strong>。这里略去数学上的证明。同理，所有依赖于「<strong>距离计算</strong>」的机器学习算法也会有此问题。</p><p>此时，我们需要把所有<strong>自变量</strong>（除了假定的x0）进行缩放、标准化，使其落在 -1 到 1 之间。最简单的方法是，置： <spanclass="math display">\[x_i^{(j)}:=\frac{x_i^{(j)}-\mu_i}{\sigma_i}\]</span> 其中，<spanclass="math inline">\(\mu_i=\frac{1}{m}\sum\limits_{j=1}^mx_i^{(j)}\)</span>是样本<strong>均值</strong>（Mean Value），<spanclass="math inline">\(\sigma_i=\sqrt{\frac{\sum\limits_{j=1}^m\left(x_i^{(j)}-\mu_i\right)^2}{m-1}}\)</span>是样本<strong>无偏标准差</strong>（UnbiasedStanderdDeviation），就完成了<strong>标准化</strong>（Standardization）。标准化后样本均值为0，方差为1，但不一定是标准正态分布（与其原始分布有关），根据中心极限定理可以推出。</p><p>需要注意的是，<strong>因变量</strong>不需要标准化，否则计算的结果将失真。且如果进行了标准化，对所有<strong>待测</strong>样本点也需要进行一样的操作，参数才能生效。</p><blockquote><p>此外，线性回归并不适用于所有情形，有时我们需要曲线来适应我们的数据，这时候我们也要对特征进行<strong>构造</strong>，如二次函数、三次函数、幂函数、对数函数等。构造后的新变量就可以当作一个新的特征来使用，这就是<strong>多项式回归</strong>（PolynomialRegression）。新变量的取值范围可能更大，此时，特征缩放就非常有必要！</p></blockquote><h3 id="归一化-normalization">归一化 | Normalization</h3><p>人们经常会混淆标准化（Standardization）与<strong>归一化</strong>（Normalization）的概念，这里也简单提一下：归一化的目的是找到某种映射关系，将原数据<strong>固定映射</strong>到某个区间<span class="math inline">\([a,b]\)</span>上，而标准化则没有限制。</p><p>归一化最常用于把有量纲数转化为<strong>无量纲数</strong>，让不同维度之间的特征在数值上有一定比较性，比如Min-Max Normalization： <span class="math display">\[x_{i}^{(j)}:=\frac{x_{i}^{(j)}-\min \left( x_i \right)}{\max \left( x_i\right) -\min \left( x_i \right)}\]</span>但是，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。因为「<strong>仅由极值决定</strong>」这个做法过于危险，如果样本中有一个异常大的值，则会将所有正常值挤占到很小的区间，而标准化方法则更加「弹性」，会兼顾所有样本。</p><h3 id="学习率-α">学习率 α</h3><p>上一节谈到，学习率（Learnigrate）的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。通过绘制<strong>迭代收敛曲线</strong>（ConvergenceGraph）可以看出学习率的好坏，也可以看出何时算法能收敛并及时<strong>终止算法</strong>。</p><p><a href="http://img.hexiaobo.xyz/cost-iter.png"><imgsrc="http://img.hexiaobo.xyz/cost-iter.png"alt="代价函数-迭代次数" /></a></p><p>代价函数-迭代次数</p><p>特别地，当 <span class="math inline">\(\alpha\)</span>取值过大时，曲线可能呈现<strong>上扬</strong>或<strong>波浪线</strong>型，解决办法都是选择更小的α 值。可以证明，只要 <spanclass="math inline">\(\alpha\)</span>足够小，凸函数都会收敛于极点。</p><p>此外，还有一种终止算法的方法：判断在某次或连续$ n $次迭代后 <spanclass="math inline">\(J(θ)\)</span>的变化小于某个极小量，如<spanclass="math inline">\(\varepsilon=1e^{-3}\)</span>，此时就可以认为算法终止。但这种办法则不能用于选择尽量大的<span class="math inline">\(α\)</span> 值。</p><h3 id="代码实现">代码实现</h3><p>下面以 <code>ex1data2.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># normalization</span><br>mu = X.mean(axis=<span class="hljs-number">0</span>)<br>sigma = X.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>X = (X - mu) / sigma<br>X = np.c_[np.ones(m), X] <span class="hljs-comment"># 增加一列 1</span><br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">1500</span><br>theta = np.zeros(n)<br>J_history = np.zeros(num_iters)<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (47, )</span><br>theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>)<br>J_history[i] = np.<span class="hljs-built_in">sum</span>(np.power(error, <span class="hljs-number">2</span>)) / (<span class="hljs-number">2</span> * m)<br><br><span class="hljs-comment"># predict</span><br>predict = (np.array([<span class="hljs-number">1650</span>, <span class="hljs-number">3</span>]) - mu) / sigma<br>predict = np.r_[<span class="hljs-number">1</span>, predict]<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br><span class="hljs-comment"># plot the convergence graph</span><br>plt.plot(np.arange(J_history.size), J_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of iterations&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost J&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1, \theta_2\right)\)</span>结果是：[340412.6595 110631.0484-6649.4724]，预测在<span class="math inline">\(\left( x_1=1650,x_2=3\right)\)</span>时的房价为 293101.0568574823。</p><p>绘制的迭代收敛曲线如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png"alt="多元线性回归的迭代收敛曲线" /></a></p><p>多元线性回归的迭代收敛曲线</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#1 梯度下降：一元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<h2 id="概论">概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that givescomputers the ability to learn <strong>without</strong> being explicitlyprogrammed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer programis said to <em>learn</em> from <strong>experience</strong> E withrespect to some <strong>task</strong> T and some <strong>performancemeasure</strong> P, if its performance on T, as measured by P, improveswith experience E.</li></ul><h3 id="监督学习-supervised-learning">监督学习 | SupervisedLearning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，SupportVector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-unsupervised-learning">无监督学习 | UnsupervisedLearning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（ClusterAlgorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-univariate-linear-regression">一元线性回归 |Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：<span class="math display">\[{(x(i),y(i)),i=1,2,⋯,m}\]</span></p><ul><li><span class="math inline">\(m\)</span> 代表训练集中实例的数量；</li><li>$x $代表<strong>特征</strong> or 输入变量；</li><li>$y $代表<strong>目标变量</strong> or 输出变量；</li><li><span class="math inline">\(({x}^{(i)},{y}^{(i)})\)</span> 代表第 i个观察实例；</li><li><span class="math inline">\(h\)</span>代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设<span class="math inline">\(h\)</span>。在本文中，我们尝试用线性函数<span class="math inline">\(h_\theta \left( x \right)=\theta_{0} +\theta_{1}x\)</span> 拟合之。因为只含有一个特征 or输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-cost-function">代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）<spanclass="math inline">\(\theta_{0}\)</span> 和 <spanclass="math inline">\(\theta_{1}\)</span>，即直线的斜率和在 <spanclass="math inline">\(y\)</span> 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modelingerror）。我们要做的就是尽量选择参数使得误差降低，即最小化 <spanclass="math inline">\(h_{\theta}(x^{(i)})\)</span> 和 <spanclass="math inline">\(h_{\theta}(y^{(i)})\)</span>的距离。于是我们有了经典的<strong>平方误差代价函数</strong>： <spanclass="math display">\[J\left( \theta _0,\theta _1 \right) =\frac{1}{2m}\sum_{i=1}^m{\left(h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}\]</span> 也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 <span class="math inline">\(\theta_0\)</span> 和 <span class="math inline">\(\theta _1\)</span> 和 <spanclass="math inline">\(J(\theta _0,\theta _1)\)</span>，对每个 <spanclass="math inline">\((\theta _0,\theta _1)\)</span>对，代入训练集可以得到一个 <span class="math inline">\(J(\theta_0,\theta_1)\)</span>值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><a href="http://img.hexiaobo.xyz/3d-surface.png"><imgsrc="http://img.hexiaobo.xyz/3d-surface.png"alt="三维空间中的碗状曲面" /></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 <spanclass="math inline">\(J(\theta_{0},\theta_{1})\)</span>最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contourplot）：</p><p><a href="http://img.hexiaobo.xyz/contour.png"><imgsrc="http://img.hexiaobo.xyz/contour.png"alt="二维空间的等高线图" /></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 <spanclass="math inline">\(J(\theta_{0}, \theta_{1})\)</span> 最小的点。</p><h3 id="梯度下降法-gradient-descent">梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数$J $取最小值的 <span class="math display">\[\left( \theta_0, \theta_1\right)\]</span>对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合<spanclass="math inline">\(\left( \theta_{0},\theta_{1},……,\theta_{n}\right)\)</span>，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（localminimum），但我们无法确定其是否就是<strong>全局最小值</strong>（globalminimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convexfunction），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：<span class="math display">\[\nabla J=\mathrm{grad} J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\}\]</span> 所以不断迭代进行赋值： <span class="math display">\[\theta_j:=\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j}J(\theta_{0}, \theta_{1})\]</span> 直到收敛，即可找到一个<strong>极小值</strong>。其中，α就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnigrate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导">数学推导</h3><p>对代价函数求偏导：</p><ul><li><span class="math inline">\(j=0\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _0}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right)}\)</span></li><li><span class="math inline">\(j=1\)</span> 时：<spanclass="math inline">\(\frac{\partial}{\partial \theta _1}J(\theta_0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{x^{(i)}\left( \theta_1x^{(i)}+\theta _0-y^{(i)} \right)}\)</span></li></ul><p>所以梯度方向为： <span class="math display">\[\mathrm{grad } J=\left\{ \frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta _1} \right\} =\left\{\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)}\right)},\frac{1}{m}\sum_{i=1}^mx^{(i)}\left( \theta _1x^{(i)}+\theta_0-y^{(i)} \right) \right\}\]</span>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如<span class="math inline">\(\sum_{i=1}^mx^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^my^{(i)}\)</span>、<spanclass="math inline">\(\sum_{i=1}^mx^{(i)}y^{(i)}\)</span>等项，这也称为<strong>批量梯度下降</strong>（BatchGradient Descent）。</p><h3 id="代码实现">代码实现</h3><p>需要注意的是，迭代赋值的过程中，<span class="math inline">\(\left(\theta_0, \theta_1\right)\)</span>的值要同时更新，否则就会与梯度方向有微小区别。用 Python元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当<spanclass="math inline">\(a\)</span>和 <spanclass="math inline">\(b\)</span>都为向量时有 <spanclass="math inline">\(a^Tb=b^Ta\)</span>，所以有： <spanclass="math display">\[X\theta =\left[ \begin{array}{c}    -\left( x^{(1)} \right) ^T\theta -\\    -\left( x^{(2)} \right) ^T\theta -\\    \vdots\\    -\left( x^{(m)} \right) ^T\theta -\\\end{array} \right] =\left[ \begin{array}{c}    -\theta ^T\left( x^{(1)} \right) -\\    -\theta ^T\left( x^{(2)} \right) -\\    \vdots\\    -\theta ^T\left( x^{(m)} \right) -\\\end{array} \right]\]</span> 下面以一元线性回归数据集 <code>ex1data1.txt</code>为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的<span class="math inline">\(\left( \theta_0, \theta_1\right)\)</span> 结果是：[-3.8953 1.1929]，作图如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><imgsrc="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归" /></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用<code>for</code>循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
