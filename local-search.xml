<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>特征选择（Feature selection）方法汇总</title>
    <link href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-selection%EF%BC%89%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB.html"/>
    <url>/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-selection%EF%BC%89%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB.html</url>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><code>特征选择</code>是<code>特征工程</code>里的一个重要问题，其目标是<strong>寻找最优特征子集</strong>。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，<strong>提高模型精确度，减少运行时间的目的</strong>。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。</p><p>之所以要考虑特征选择，是因为机器学习经常面临过拟合的问题。 <strong>过拟合</strong>的表现是模型参数<strong>太贴合训练集数据</strong>，模型在训练集上效果很好而在测试集上表现不好，也就是在高方差。简言之模型的泛化能力差。过拟合的原因是模型对于训练集数据来说太复杂，要解决过拟合问题，一般考虑如下方法：</p><ol><li>收集更多数据</li><li>通过正则化引入对复杂度的惩罚</li><li>选择更少参数的简单模型</li><li>对数据降维（降维有两种方式：特征选择和特征抽取）</li></ol><p>其中第1条一般是很难做到的，一般主要采用第2和第4点</p><h2 id="一般流程"><a href="#一般流程" class="headerlink" title="一般流程"></a>一般流程</h2><p>特征选择的一般过程：</p><ol><li>生成子集：搜索特征子集，为评价函数提供特征子集</li><li>评价函数：评价特征子集的好坏</li><li>停止准则：与评价函数相关，一般是阈值，评价函数达到一定标准后就可停止搜索</li><li>验证过程：在验证数据集上验证选出来的特征子集的有效性</li></ol><p>但是， 当特征数量很大的时候， 这个搜索空间会很大，如何找最优特征还是需要一些经验结论。</p><h2 id="三大类方法"><a href="#三大类方法" class="headerlink" title="三大类方法"></a>三大类方法</h2><p>根据特征选择的形式，可分为三大类：</p><ul><li>Filter(过滤法)：按照<code>发散性</code>或<code>相关性</code>对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选</li><li>Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征</li><li>Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）</li></ul><h2 id="过滤法"><a href="#过滤法" class="headerlink" title="过滤法"></a>过滤法</h2><p>基本想法是：分别对每个特征 $x_i$ ，计算 $x_i$ 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征$x_i$ 。</p><ul><li>Pearson相关系数</li><li>卡方验证</li><li>互信息和最大信息系数</li><li>距离相关系数</li><li>方差选择法</li></ul><h3 id="Pearson相关系数"><a href="#Pearson相关系数" class="headerlink" title="Pearson相关系数"></a>Pearson相关系数</h3><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，衡量的是变量之间的线性相关性，结果的取值区间为<strong>[-1,1]</strong> ， -1 表示完全的负相关(这个变量下降，那个就会上升)， +1 表示完全的正相关， 0 表示没有线性相关性。Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的<a href="https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html">pearsonr</a>方法能够同时计算相关系数和p-value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> pearsonr<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>size = <span class="hljs-number">300</span><br>x = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Lower noise：&quot;</span>, pearsonr(x, x + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Higher noise：&quot;</span>, pearsonr(x, x + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size)))<br><br><br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-comment"># 选择K个最好的特征，返回选择特征后的数据</span><br><span class="hljs-comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数</span><br><span class="hljs-comment"># 参数k为选择的特征个数</span><br>SelectKBest(<span class="hljs-keyword">lambda</span> X, Y: array(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><p>Pearson相关系数的一个<strong>明显缺陷</strong>是，作为特征排序机制，他<strong>只对线性关系敏感</strong>。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近 0 。</p><h3 id="卡方验证"><a href="#卡方验证" class="headerlink" title="卡方验证"></a>卡方验证</h3><p>经典的卡方检验是检验<strong>类别型变量</strong>对<strong>类别型变量</strong>的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：</p><script type="math/tex; mode=display">χ2=∑(A−E)2E</script><p>不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用sklearn中feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> chi2<br>iris = load_iris()<br>X, y = iris.data, iris.target  <span class="hljs-comment">#iris数据集</span><br><br><span class="hljs-comment">#选择K个最好的特征，返回选择特征后的数据</span><br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><p><a href="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/classes.html%23module-sklearn.feature_selection">sklearn.feature_selection</a>模块中的类可以用于样本集中的特征选择/维数降低，以提高估计器的准确度分数或提高其在非常高维数据集上的性能</p><h3 id="互信息和最大信息系数-Mutual-information-and-maximal-information-coefficient-MIC"><a href="#互信息和最大信息系数-Mutual-information-and-maximal-information-coefficient-MIC" class="headerlink" title="互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)"></a>互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)</h3><p>经典的互信息也是评价<strong>类别型变量</strong>对<strong>类别型变量</strong>的相关性的，互信息公式如下：</p><script type="math/tex; mode=display">MI(x_i,y)=∑x_i∈0,1∑y∈0,1 \  p(x_i,y)logp(x_i,y)p(x_i)p(y)</script><p>当$x_i$是0/1离散值的时候，这个公式如上。很容易推广到 $x_i$ 是多个离散值的情况。这里的 $p(x_i,y) , p(x_i)$ 和 p(y) 都是从训练集上得到的。若问这个 MI 公式如何得来，请看它的 KL 距离（Kullback-Leibler）表述： $MI(x_i,y)=KL(P(x_i,y)||p(x_i)p(y)) $也就是说, MI 衡量的是 $x_i$ 和 $y$ 的独立性。如果它俩独立 $P(x_i,y)=p(x_i)p(y)$ ，那么 KL 距离值为0，也就是 $x_i$ 和 $y$ 不相关了，可以去除 $x_i$。相反，如果两者密切相关，那么 MI 值会很大。在对 MI 进行排名后，最后剩余的问题就是如何选择 k 个值（前 k 个 $x_i$ ）。(后面将会提到此方法)我们继续使用交叉验证的方法，将 k 从 1 扫描到 n ，取评分最高的k 。 不过这次复杂度是线性的了。比如，在使用朴素贝叶斯分类文本的时候，词表长度 n 很大。 使用filiter特征选择方法，能够增加分类器精度。</p><p>想把互信息直接用于特征选择其实不是太方便：</p><ol><li>它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较</li><li>对于连续变量的计算不是很方便（ X 和 Y 都是集合, $x_i$, $y$ 都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感</li></ol><p><strong>最大信息系数</strong>克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在 [0,1] 。<a href="https://link.zhihu.com/?target=https%3A//minepy.readthedocs.io/en/latest/">minepy</a>提供了MIC功能。</p><p>下面我们来看下 $y=x^2$ 这个例子，MIC算出来的互信息值为1(最大的取值)。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> minepy <span class="hljs-keyword">import</span> MINE<br><br>m = MINE()<br>x = np.random.uniform(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10000</span>)<br>m.compute_score(x, x**<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(m.mic())<br><br><br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-comment">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mic</span>(<span class="hljs-params">x, y</span>):<br>    m = MINE()<br>    m.compute_score(x, y)<br>    <span class="hljs-keyword">return</span> (m.mic(), <span class="hljs-number">0.5</span>)<br><span class="hljs-comment"># 选择K个最好的特征，返回特征选择后的数据</span><br>SelectKBest(<span class="hljs-keyword">lambda</span> X, Y: array(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><h3 id="距离相关系数"><a href="#距离相关系数" class="headerlink" title="距离相关系数"></a>距离相关系数</h3><p>距离相关系数是为了克服Pearson相关系数的弱点而生的。在 $x$ 和 $x^2$ 这个例子中，即便Pearson相关系数是 0 ，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是 0 ，那么我们就可以说这两个变量是独立的。</p><p>R的<a href="https://link.zhihu.com/?target=https%3A//cran.r-project.org/web/packages/energy/index.html">energy</a>包里提供了距离相关系数的实现，另外这是<a href="https://link.zhihu.com/?target=https%3A//gist.github.com/josef-pkt/2938402">Python gist</a>的实现。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text"># R-code<br>&gt; x = runif (1000, -1, 1)<br>&gt; dcor(x, x**2)<br>[1] 0.4943864<br></code></pre></td></tr></table></figure><p>尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。</p><p>第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。</p><p>第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</p><h3 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h3><p>过滤特征选择法还有一种方法不需要度量特征 $x_i$ 和类别标签 $y$ 的信息量。这种方法先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p><p>例如，假设我们有一个具有布尔特征的数据集，并且我们要删除超过80％的样本中的一个或零（开或关）的所有特征。布尔特征是伯努利随机变量，这些变量的方差由下式给出: $Var[X]=p(1−p)$</p><p><a href="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html%23sklearn.feature_selection.VarianceThreshold">VarianceThreshold</a>是特征选择的简单基线方法。它删除方差不符合某个阈值的所有特征。默认情况下，它会删除所有零差异特征，即所有样本中具有相同值的特征。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br>X = [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]<br><span class="hljs-comment"># 方差选择法，返回值为特征选择后的数据</span><br><span class="hljs-comment"># 参数threshold为方差的阈值</span><br>sel = VarianceThreshold(threshold=(<span class="hljs-number">.8</span> * (<span class="hljs-number">1</span> - <span class="hljs-number">.8</span>)))<br><span class="hljs-built_in">print</span>(sel.fit_transform(X))<br><br><span class="hljs-comment"># VarianceThreshold(threshold=3).fit_transform(iris.data)</span><br></code></pre></td></tr></table></figure><p>输出结果：</p><p>array([[0, 1],  [1, 0],  [0, 0],  [1, 1],  [1, 0],  [1, 1]]) 如预期的那样，VarianceThreshold已经删除了第一列，其具有 $p=5/6&gt;0.8$ 包含零的概率。</p><p>方差选择的逻辑并不是很合理，这个是基于各特征分布较为接近的时候，才能以方差的逻辑来衡量信息量。但是如果是离散的或是仅集中在几个数值上，如果分布过于集中，其信息量则较小。而对于连续变量，由于阈值可以连续变化，所以信息量不随方差而变。 实际使用时，可以结合cross-validate进行检验</p><h2 id="包装法"><a href="#包装法" class="headerlink" title="包装法"></a>包装法</h2><p>基本思想：基于hold-out方法，对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集。需要先选定特定算法，通常选用普遍效果较好的算法， 例如Random Forest， SVM， kNN等等。</p><blockquote><p>西瓜书上说包装法应该欲训练什么算法，就选择该算法进行评估<br>随着学习器（评估器）的改变，最佳特征组合可能会改变</p></blockquote><p>贪婪搜索算法（greedy search）是局部最优算法。与之对应的是穷举算法 (exhaustive search)，穷举算法是遍历所有可能的组合达到全局最优级，但是计算复杂度是2^n，一般是不太实际的算法。</p><h3 id="前向搜索"><a href="#前向搜索" class="headerlink" title="前向搜索"></a>前向搜索</h3><p>前向搜索说白了就是，每次增量地从剩余未选中的特征选出一个加入特征集中，待达到阈值或者 n 时，从所有的 F 中选出错误率最小的。过程如下：</p><ol><li>初始化特征集 F 为空。</li><li>扫描 i 从 1 到 n 如果第 i 个特征不在 $F$ 中，那么特征 i 和F 放在一起作为 $F_i$ (即$ F_i=F\cup{i} )$。 在只使用 $F_i$中特征的情况下，利用交叉验证来得到 F_i 的错误率。</li><li>从上步中得到的 n 个 $F_i$ 中选出错误率最小的 $F_i$ ,更新 F 为 $F_i$。</li><li>如果 F 中的特征数达到了 n 或者预定的阈值（如果有的话）， 那么输出整个搜索过程中最好的 ；若没达到，则转到 2，继续扫描。</li></ol><h3 id="后向搜索"><a href="#后向搜索" class="headerlink" title="后向搜索"></a>后向搜索</h3><p>既然有增量加，那么也会有增量减，后者称为后向搜索。先将 F 设置为 ${1,2,…,n}$ ，然后每次删除一个特征，并评价，直到达到阈值或者为空，然后选择最佳的 F 。</p><p>这两种算法都可以工作，但是计算复杂度比较大。时间复杂度为：O(n+(n−1)+(n−2)+…+1)=O(n2)</p><h3 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h3><p>递归消除特征法使用一个<code>基模型</code>来进行多轮训练，每轮训练后通过学习器返回的 coef_ 或者feature_importances_ 消除若干权重较低的特征，再基于新的特征集进行下一轮训练。</p><p>使用feature_selection库的RFE类来选择特征的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br><span class="hljs-comment">#递归特征消除法，返回特征选择后的数据</span><br><span class="hljs-comment">#参数estimator为基模型</span><br><span class="hljs-comment">#参数n_features_to_select为选择的特征个数</span><br>RFE(estimator=LogisticRegression(), n_features_to_select=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><h2 id="嵌入法"><a href="#嵌入法" class="headerlink" title="嵌入法"></a>嵌入法</h2><ul><li>基于惩罚项的特征选择法 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br><span class="hljs-comment">#带L1惩罚项的逻辑回归作为基模型的特征选择   </span><br>SelectFromModel(LogisticRegression(penalty=<span class="hljs-string">&quot;l1&quot;</span>, C=<span class="hljs-number">0.1</span>)).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><p>要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><br><span class="hljs-comment">#带L1和L2惩罚项的逻辑回归作为基模型的特征选择   </span><br><span class="hljs-comment">#参数threshold为权值系数之差的阈值   </span><br>SelectFromModel(LR(threshold=<span class="hljs-number">0.5</span>, C=<span class="hljs-number">0.1</span>)).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><ul><li>基于学习模型的特征排序 这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。通过这种训练对特征进行打分获得相关性后再训练最终模型。</li></ul><p>在<a href="https://link.zhihu.com/?target=https%3A//archive.ics.uci.edu/ml/datasets/Housing">波士顿房价数据集</a>上使用sklearn的<a href="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">随机森林回归</a>给出一个<strong>单变量选择</strong>的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> cross_val_score, ShuffleSplit<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_boston<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor<br><br><span class="hljs-comment">#加载波士顿房价作为数据集</span><br>boston = load_boston()<br>X = boston[<span class="hljs-string">&quot;data&quot;</span>]<br>Y = boston[<span class="hljs-string">&quot;target&quot;</span>]<br>names = boston[<span class="hljs-string">&quot;feature_names&quot;</span>]<br><br><span class="hljs-comment">#n_estimators为森林中树木数量，max_depth树的最大深度</span><br>rf = RandomForestRegressor(n_estimators=<span class="hljs-number">20</span>, max_depth=<span class="hljs-number">4</span>)<br>scores = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>    <span class="hljs-comment">#每次选择一个特征，进行交叉验证，训练集和测试集为7:3的比例进行分配，</span><br>    <span class="hljs-comment">#ShuffleSplit()函数用于随机抽样（数据集总数，迭代次数，test所占比例）</span><br>    score = cross_val_score(rf, X[:, i:i+<span class="hljs-number">1</span>], Y, scoring=<span class="hljs-string">&quot;r2&quot;</span>,<br>                               cv=ShuffleSplit(<span class="hljs-built_in">len</span>(X), <span class="hljs-number">3</span>, <span class="hljs-number">.3</span>))<br>    scores.append((<span class="hljs-built_in">round</span>(np.mean(score), <span class="hljs-number">3</span>), names[i]))<br><br><span class="hljs-comment">#打印出各个特征所对应的得分</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">sorted</span>(scores, reverse=<span class="hljs-literal">True</span>))<br></code></pre></td></tr></table></figure><p>输出结果：</p><p>[(0.64300000000000002, ‘LSTAT’), (0.625, ‘RM’), (0.46200000000000002, ‘NOX’), (0.373, ‘INDUS’), (0.30299999999999999, ‘TAX’), (0.29799999999999999, ‘PTRATIO’), (0.20399999999999999, ‘RAD’), (0.159, ‘CRIM’), (0.14499999999999999, ‘AGE’), (0.097000000000000003, ‘B’), (0.079000000000000001, ‘ZN’), (0.019, ‘CHAS’), (0.017999999999999999, ‘DIS’)]</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征选择（Feature selection）</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#4 逻辑回归：二分类到多分类</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB.html</url>
    
    <content type="html"><![CDATA[<p>前文介绍了基本的线性回归问题，尝试预测一系列<strong>连续值</strong>属性。现在我们将介绍的分类问题则关注<strong>离散值</strong>属性的预测。</p><p>在分类问题中，我们尝试预测的结果是否属于某一个类，最基础的就是<strong>二元</strong>的分类问题（例如预测垃圾邮件、恶性肿瘤），更为复杂的则是预测<strong>多元</strong>的分类问题。</p><h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><p>分类问题的样本与回归问题类似，由特征和目标构成，给定数据集：</p><script type="math/tex; mode=display">\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}</script><ul><li>$x(i)$ 代表第 $i$ 个观察实例的 $n+1$ 维<strong>特征向量</strong> $\left(x_0^{(i)},\cdots,x_n^{(i)}\right)^T$；</li><li>$y^{(i)}\in\{0,1\}$ 代表第 $i$ 个观察实例的<strong>目标变量</strong>，在这里有 $0$ 或 $1$ 两类结果。</li></ul><p>即对于输入的自变量 $x^(i)$，因变量 $y^(i)$ 可能为 $0$ 或 $1$。其中 $0$ 表示<strong>负向类</strong>（negative class），$1$ 表示<strong>正向类</strong>（positive class）。</p><blockquote><p>我们不对「正向」和「负向」加以特殊区分，但在实际应用中「正向」通常表示「具有我们要寻找的东西」，如垃圾邮件、恶性肿瘤等。</p></blockquote><h3 id="线性回归的不足"><a href="#线性回归的不足" class="headerlink" title="线性回归的不足"></a>线性回归的不足</h3><p>首先可能会自然而然地想到用之前的线性回归来解决——用一条直线拟合结果，当预测值大于 0.5 时归为正向类，反之归为负向类。</p><p>这看似合理，然而，线性回归保留了 $y^(i)$ 太多的「<strong>信息量</strong>」。对于某些「<strong>反常样本</strong>」，我们可能预测出一个远大于 $1$ 或者远小于 $0$ 的结果，同理，这些「反常样本」用于拟合直线时也会对其造成一定偏移，以至于正常样本被归为错误类别。</p><p><img src="http://img.hexiaobo.xyz/linear-regression.png" alt="反常样本使得蓝线偏移"></p><h2 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归 | Logistic Regression"></a>逻辑回归 | Logistic Regression</h2><p>线性回归和逻辑回归都属于<strong>广义线性模型</strong>（Generalized Linear Model）的特殊形式，线性模型都可用于回归问题的求解。但由于<strong>逻辑函数</strong>（Losistic Function）将结果映射到 <strong>Bernoulli 分布</strong>，因此逻辑回归更常用于分类问题。</p><h3 id="假说表示-Hypothesis-Representation"><a href="#假说表示-Hypothesis-Representation" class="headerlink" title="假说表示 | Hypothesis Representation"></a>假说表示 | Hypothesis Representation</h3><p>回忆线性回归的假设函数：$h_\theta(x)=\theta^Tx$，我们在其外套上 $sigmoid$ 函数，构造逻辑回归的假设函数为：</p><script type="math/tex; mode=display">h_\theta(x)=g\left(\theta^Tx\right)=\frac{1}{1+e^{-\theta^T x}}</script><blockquote><p>所谓 sigmoid 函数（也即前面提到的<strong>逻辑函数</strong>）：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p><img src="http://img.hexiaobo.xyz/sigmoid.png" alt="sigmoid函数"></p><p>是一个介于 $(0,1)$之间的单增 $S$ 形函数，其导出需要用到 GLMs 和指数分布族（The Exponential Family）的知识。</p></blockquote><p>也就是说，对于一个参数为 $θ$ 的逻辑回归模型，输入 $x$，得到 $h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$ 的预测值。</p><p>我们可以把这个输出值视为 $x$ 这个样本对应的 $y$ 等于 1 的<strong>概率</strong>（estimated probablity），即 $h_\theta \left( x \right)=P\left( y=1|x;\theta \right)$。针对分类情形，我们可以认为如果概率 $\geqslant 0.5$，则分类为 $1$，否则分类为 $0$。</p><h3 id="决策边界-Decision-Boundary"><a href="#决策边界-Decision-Boundary" class="headerlink" title="决策边界 | Decision Boundary"></a>决策边界 | Decision Boundary</h3><p>又根据 $sigmoid$ 函数的性质:</p><script type="math/tex; mode=display">h_\theta(x)\geqslant 0.5\iff \theta^Tx\geqslant0</script><p>所以只要 $\theta^Tx\geqslant0$，就会分类为 $1$，否则分类为 $0$；于是乎，$\theta^Tx=0$ 解出的这条「<strong>线</strong>」（对于高维情形为<strong>超平面</strong>）被称作决策边界，它将整个空间划分成两块区域（region），各自属于一个分类。</p><p>下面看两个二维情形的例子：</p><p><img src="http://img.hexiaobo.xyz/decision-boundary-1.png" alt="线性的决策边界"></p><p>对于上述样本点的分布，用一条直线即可划分空间，对应的假设函数为 $h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2 x_2\right)$。</p><p><img src="http://img.hexiaobo.xyz/decision-boundary-2.png" alt="多项式的决策边界"></p><p>而对于这种分布，我们必须选择二维曲线来划分空间，即使用<strong>多项式特征</strong>来确定曲线的参数，对应的假设函数为 $h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2 x_2+\theta_3 x_3^2+\theta_4 x_4^2\right)$。当然，我们也可以用更复杂的多项式曲线来划分更复杂的分布。</p><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>现在，我们的任务就是从训练集中拟合逻辑回归的参数 $θ$。仍然采用代价函数的思想——找到使代价最小的参数即可。</p><p>广义上来讲，代价函数是这样的一个函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)</script><p>也就是说用每个数据的估计值 $h_\theta(x^{(i)})$ 和真实值 $y^(i)$ 计算一个代价 $\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)$，比如线性回归中这个代价就是二者差值的平方。</p><p>理论上来说，我们也可以对逻辑回归模型沿用平方误差的定义，但当我们将 ${h_\theta}\left( x \right)=\frac{1}{1+{e^{-\theta^{T}x}}}$ 代入到这样的代价函数中时，我们得到的将是一个<strong>非凸函数</strong>（non-convex function）。这意味着空间中会有许多<strong>局部最小值</strong>，使得梯度下降法难以寻找到<strong>全局最小值</strong>。</p><p>因此我们重新定义逻辑回归的<strong>代价函数</strong>：</p><script type="math/tex; mode=display">\mathrm{Cost}\left( h_{\theta}(x),y \right) =\begin{cases}    -\ln \left( h_{\theta}\left( x \right) \right)&        y=1\\    -\ln \left( 1-h_{\theta}\left( x \right) \right)&        y=0\\\end{cases}</script><p>绘制出的曲线大致呈这样：</p><p><img src="http://img.hexiaobo.xyz/cost.png" alt="代价函数"></p><p>观察曲线，发现当 $y=1$（样本的真实值为 $1$）时，预测值 $h_\theta(x)$ 越接近 $1$ 则代价越小，越接近 $0$ 则代价趋于无穷。譬如在肿瘤分类中，将实际为恶性的肿瘤以百分之百的概率预测为良性，带来的后果将不可估量。</p><p>与此同时，注意到代价函数也可以<strong>简写</strong>为：</p><script type="math/tex; mode=display">\mathrm{Cost}\left( h_{\theta}\left( x \right) ,y \right) =-\left[ y\ln \left( h_{\theta}\left( x \right) \right) +\left( 1-y \right) \ln \left( 1-h_{\theta}\left( x \right) \right) \right]</script><p>它还有另外一个名称——<strong>二元交叉熵代价函数</strong>（BCE, Binary Cross-Entropy），它又蕴含着怎样的原理呢？</p><h3 id="代价函数的数学推导"><a href="#代价函数的数学推导" class="headerlink" title="代价函数的数学推导"></a>代价函数的数学推导</h3><p>首先明确什么是一个<strong>好的代价函数</strong>——当参数 $θ$ 使得 $J(θ)$ 取<strong>极小值</strong>时，这个 $θ$ 也能使模型拟合效果最好。这时我们回忆起 <strong>极大似然估计</strong> 的思想：当参数 $θ$ 使得 $L(θ)$ 取<strong>极大值</strong>时，这个 $θ$ 也能使得<strong>事件组</strong>最容易发生！</p><p>前文已经提到，我们用概率解释预测值 $h_\theta(x)=P(y=1)$，于是 $1-h_\theta(x)=P(y=0)$，故：</p><script type="math/tex; mode=display">P\left( y=k \right) =\left[ h_{\theta}\left( x \right) \right] ^k\left[ 1-h_{\theta}\left( x \right) \right] ^{1-k},\quad k\in \left\{ 0,1 \right\}</script><p>而对于数据集 $\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}$ 下，将其视为已发生的一个<strong>事件组</strong>，则似然函数为：</p><script type="math/tex; mode=display">L\left( \theta \right) =\prod_{i=1}^m{P}\left( y=y^{(i)} \right) =\prod_{i=1}^m{\left[ h_{\theta}\left( x^{(i)} \right) \right] ^{y^{(i)}}}\left[ 1-h_{\theta}\left( x^{(i)} \right) \right] ^{1-y^{(i)}}</script><p>取对数得到：</p><script type="math/tex; mode=display">\ln L(\theta )=\sum_{i=1}^m{\left\{ y^{(i)}\ln \left[ h_{\theta}\left( x^{(i)} \right) \right] +\left( 1-y^{(i)} \right) \ln \left[ 1-h_{\theta}\left( x^{(i)} \right) \right] \right\}}</script><p>注意到，极大似然法的目标是找到 $L(θ)$ 或 $\ln L(\theta)$ 的极大值，而逻辑回归的目标是找到 $J(θ)$ 的极小值，所以自然的，我们将 $ln⁡ L(θ)$ <strong>取反</strong>来定义 $J(θ)$：</p><script type="math/tex; mode=display">\begin{aligned}J(\theta)&=-\frac{1}{m}\ln L(\theta)\\&=-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left(i\right)}\ln \left( h_{\theta}\left( x^{\left(i\right)} \right) \right) +\left( 1-y^{\left(i\right)} \right) \ln \left( 1-h_{\theta}\left( x^{\left(i\right)} \right) \right) \right]}\end{aligned}</script><p>其中 $\frac{1}{m}$ 对要求的 $\theta$ 没有影响，仅是取一下平均罢了。</p><blockquote><p>可以证明上述代价函数 $J(θ)$ 会是一个<strong>凸函数</strong>，并且没有局部最优值。凸性分析的内容不在本讲的范围，但是可以证明我们所选的代价函数会给我们带来一个<strong>凸优化</strong>问题（Convex Optimization）。</p></blockquote><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>既然是凸函数，那么现在我们就可以进行梯度下降求解 $\underset{\theta}{\arg\min }J\left( \theta \right)$ 。</p><p>为了求偏导，我们先计算：</p><script type="math/tex; mode=display">\begin{aligned}    \frac{\partial}{\partial \theta}\mathrm{Cost}\left( h_{\theta}\left( x \right) ,y \right) &=\frac{\partial}{\partial \theta}\left[ -y\ln \left( h_{\theta}\left( x \right) \right) -\left( 1-y \right) \ln \left( 1-h_{\theta}\left( x \right) \right) \right]\\    &=\frac{\partial}{\partial \theta}\left[ y\ln \left( 1+e^{-\theta ^Tx} \right) +(1-y)\ln \left( 1+e^{\theta ^Tx} \right) \right]\\    &=\frac{-yxe^{-\theta ^Tx}}{1+e^{-\theta ^Tx}}+\frac{\left( 1-y \right) xe^{\theta ^Tx}}{1+e^{\theta ^Tx}}\\    &=\frac{-yx+\left( 1-y \right) xe^{\theta ^Tx}}{1+e^{\theta ^Tx}}\\    &=\left( -y+\frac{1}{1+e^{-\theta ^Tx}} \right) x\\    &=\left( h_{\theta}\left( x \right) -y \right) x\\\end{aligned}</script><p>于是乎，</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) x^{\left( i \right)}}</script><p>没错，这个偏导的形式和线性回归完全相同！不同的只是 ${h_\theta}\left( x \right)=g\left( {\theta^T}X \right)$ 的定义——多了一层 $sigmoid$ 函数，正是因此，我们不能使用正规方程直接给出<strong>解析解</strong>，而必须使用梯度下降等方法。</p><script type="math/tex; mode=display">\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}</script><p>现在我们对其使用梯度下降即可。另外，在运行梯度下降算法之前，进行<strong>特征缩放</strong>依旧是非常必要的。</p><blockquote><p>除了梯度下降法，还有很多算法可以用来求解这个最优值：共轭梯度法（Conjugate Gradient）、局部优化法（Broyden fletcher goldfarb shann, BFGS）、有限内存局部优化法（LBFGS）等。</p><p>这些算法通常不需要手动选择学习率 $α$，而是使用一个智能的内循环（线性搜索算法）来选择一个较好的 $α$，甚至能为每次迭代选择不同的 $α$。因此他们有着更优越的常数和时间复杂度，在大型机器学习项目中更加适用。</p></blockquote><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面以 <a href="https://www.coursera.org/">Coursera</a> 上的二分类数据集 <code>ex2data1.txt</code> 为例，首先看一下数据的分布：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-meta"># load data, data.shape = (100, 3)</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = np.loadtxt(&#x27;<span class="hljs-title">ex2data1</span>.<span class="hljs-title">txt&#x27;</span>, <span class="hljs-title">delimiter</span>=&#x27;,&#x27;)</span><br>(m, n) = <span class="hljs-class"><span class="hljs-keyword">data</span>.shape</span><br><span class="hljs-type">X</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>[:, :-1]</span><br><span class="hljs-title">y</span> = <span class="hljs-class"><span class="hljs-keyword">data</span>[:, -1]</span><br><br><span class="hljs-meta"># preview data</span><br><span class="hljs-title">pos</span> = np.<span class="hljs-keyword">where</span>(y == <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]<br><span class="hljs-title">neg</span> = np.<span class="hljs-keyword">where</span>(y == <span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]  # 返回索引<br><span class="hljs-title">plt</span>.scatter(<span class="hljs-type">X</span>[pos, <span class="hljs-number">0</span>], <span class="hljs-type">X</span>[pos, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;o&quot;</span>, c=&#x27;c&#x27;)<br><span class="hljs-title">plt</span>.scatter(<span class="hljs-type">X</span>[neg, <span class="hljs-number">0</span>], <span class="hljs-type">X</span>[neg, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&quot;x&quot;</span>, c=&#x27;r&#x27;)<br><span class="hljs-title">plt</span>.xlabel(&#x27;<span class="hljs-type">Exam</span> <span class="hljs-number">1</span> score&#x27;)<br><span class="hljs-title">plt</span>.ylabel(&#x27;<span class="hljs-type">Exam</span> <span class="hljs-number">2</span> score&#x27;)<br><span class="hljs-title">plt</span>.show()<br><br><span class="hljs-type">PYTHON</span><br></code></pre></td></tr></table></figure><p><img src="http://img.hexiaobo.xyz/preview_scatter.png" alt="数据分布散点图"></p><p>看起来用<strong>直线</strong>即可划分数据。此外，注意到如果每次都用 <code>np.sum()</code> 计算 $\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j$ 耗时较大，因此将求和化成<strong>矩阵形式</strong>：</p><script type="math/tex; mode=display">\theta :=\theta -\alpha \frac{1}{m}X^T\left( g\left( X\theta \right) -y \right)</script><p>实现逻辑回归如下，矩阵化后运行时间可缩短一半：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br>import matplotlib.pyplot as plt<br><br><span class="hljs-comment"># load data, data.shape = (100, 3)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex2data1.txt&#x27;</span>, <span class="hljs-attribute">delimiter</span>=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-1]<br>y = data[:, -1]<br><br><span class="hljs-comment"># normalization</span><br>X = (X - X.mean(<span class="hljs-attribute">axis</span>=0)) / X.std(<span class="hljs-attribute">axis</span>=0, <span class="hljs-attribute">ddof</span>=1)<br>X = np.c_[np.ones(m), X] # 增加一列 1<br><br><span class="hljs-comment"># parameters</span><br>alpha = 0.01<br>num_iters = 10000<br>theta = np.zeros(n)<br><br>def sigmoid(z):<br>    g = np.zeros(z.size)<br>    g = 1 / (1 + np.exp(-z))<br>    return g<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(0, num_iters):<br><span class="hljs-built_in">error</span> = sigmoid(X @ theta) - y  # error.shape = (100, )<br>theta -= (alpha / m) * (X.T @ error)  # X.T.shape = (2, 100)<br><br><span class="hljs-comment"># plot decision boundary</span><br>pos = np.where(y == 1)[0]<br>neg = np.where(y == 0)[0]<br>plt.scatter(X[pos, 1], X[pos, 2], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;o&quot;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;c&#x27;</span>)<br>plt.scatter(X[neg, 1], X[neg, 2], <span class="hljs-attribute">marker</span>=<span class="hljs-string">&quot;x&quot;</span>, <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;r&#x27;</span>)<br><br>x_plot = np.array([np.min(X[:, 1]), np.max(X[:, 1])])<br>y_plot = (-1 / theta[2]) * (theta[1] * x_plot + theta[0])<br>plt.plot(x_plot, y_plot)<br>plt.show()<br><br>PYTHON<br></code></pre></td></tr></table></figure><p>得到的 $\left( \theta_0, \theta_1, \theta_2 \right)$ 结果是：[1.2677 3.0555 2.8289]，绘制出决策边界的图像为：</p><p><img src="http://img.hexiaobo.xyz/decision_boundary.png" alt="决策边界（归一化）"></p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>在实际情形中，我们还会使用逻辑回归来解决<strong>多元</strong>的分类问题。多分类的数据集和二分类相似，区别在于<strong>目标变量</strong> $y^(i)$ 在这里不仅有 $0$ 或 $1$ 两类结果，还可以取 $2、3$ 等更多的数字。</p><p><img src="http://img.hexiaobo.xyz/multi-class.png" alt="二分类和多分类"></p><blockquote><p>对于接下来要介绍的方法，标签数字的顺序、取法，都不会影响最终的结果。但在某些分类模型中，数值可能具有实际意义，这时候使用<strong>独热码</strong>（One-Hot）或许是更好的选择。</p></blockquote><h3 id="一对余-One-vs-Rest"><a href="#一对余-One-vs-Rest" class="headerlink" title="一对余 | One vs. Rest"></a>一对余 | One vs. Rest</h3><p>对于 $N$ 分类问题，我们可以将其转化为 $N$ 个二分类问题——只需创建 $N$ 个「<strong>伪训练集</strong>」，每个训练集中仅包含一个类作为正向类，其他 $N−1$ 个类均视为负向类。</p><p><img src="http://img.hexiaobo.xyz/one-vs-rest.png" alt="One vs. Rest"></p><p>接下来就可以训练 $N$ 个标准的逻辑回归分类器，将其记为：</p><script type="math/tex; mode=display">h_\theta^{\left( i \right)}\left( x \right)=P\left( y=i|x;\theta  \right) \;\; i=\left( 1,2,\cdots,N \right)</script><p>显然，每个分类器的输出都可以视为「<strong>属于某类</strong>」的概率，在预测时，我们只需要运行一遍所有分类器，然后取其最大值作为预测结果即可。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#3 正规方程：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>前文提到线性回归问题时，我们说在数学上也可以用最小二乘法（Least Square Method）来解决，实际上其思路也是<strong>最小化</strong>平方误差代价函数（也称<strong>残差函数</strong>）。</p><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，最小二乘法的矩阵解法——正规方程则是更好的解决方案。</p><h2 id="正规方程-Normal-Equation"><a href="#正规方程-Normal-Equation" class="headerlink" title="正规方程 | Normal Equation"></a>正规方程 | Normal Equation</h2><p>利用多元微分学知识，我们知道对于代价函数：</p><script type="math/tex; mode=display">J(\theta)=J(\theta_0, \theta_1,\cdots, \theta_n)</script><p>如果它是<strong>连续</strong>的，则要求出它的最小值，只需要令各偏导为零：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta_j}=0,\quad j=0,1,\cdots,n</script><p>或写作向量形式：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\vec 0</script><p>就能解出令 $J(\theta)$ 最小化的 $θ$ 值。</p><p>由此，我们将代价函数转化为<strong>有确定解的代数方程组</strong>（其方程式数目正好等于未知数的个数），这个方程组就是正规方程（Normal Equation）。</p><h3 id="数学推导-1"><a href="#数学推导-1" class="headerlink" title="数学推导 1"></a>数学推导 1</h3><p>下面我们就对多元线性回归的代价函数进行求解：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2</script><p>于是其偏导函数为：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}</script><p>要使之为<strong>零向量</strong>，只能是：</p><script type="math/tex; mode=display">\theta^Tx^{(i)}=y^{(i)},\quad i=1,2,\cdots,m</script><p>恒成立。写作矩阵为：</p><script type="math/tex; mode=display">\theta ^TX^T=y^T\text{ ，或 }X\theta =y</script><p>其中，</p><script type="math/tex; mode=display">X_{(n+1)\times m}=\left[ \begin{matrix}    x_{0}^{(1)}&        x_{1}^{(1)}&        \cdots&        x_{n}^{(1)}\\    x_{0}^{(2)}&        x_{1}^{(2)}&        \cdots&        x_{n}^{(2)}\\    \vdots&        \vdots&        \ddots&        \vdots\\    x_{0}^{(m)}&        x_{1}^{(m)}&        \cdots&        x_{n}^{(m)}\\\end{matrix} \right] =\left[ \begin{array}{c}    {x^{(1)}}^T\\    {x^{(2)}}^T\\    \vdots\\    {x^{(m)}}^T\\\end{array} \right] ,\quad y=\left[ \begin{array}{c}    y^{(1)}\\    y^{(2)}\\    \vdots\\    y^{(m)}\\\end{array} \right]</script><p>两边同时乘以 $X^T$，假设 $X^T$可逆，解得：</p><script type="math/tex; mode=display">\theta=(X^TX)^{-1}X^Ty</script><h3 id="数学推导-2"><a href="#数学推导-2" class="headerlink" title="数学推导 2"></a>数学推导 2</h3><p>前面的推导中，在<strong>向量形式</strong>的偏导函数中发现了简化条件，将零向量提出来单独求解。下面介绍另一个<strong>纯矩阵形式</strong>的解法。</p><p>首先将代价函数表示为：</p><script type="math/tex; mode=display">\begin{aligned}    J(\theta )&=\frac{1}{2m}\left( X\theta -y \right) ^T\left( X\theta -y \right)\\    &=\frac{1}{2}\left( \theta ^TX^T-y^T \right) \left( X\theta -y \right)\\    &=\frac{1}{2}\left( \theta ^TX^TX\theta -\theta ^TX^Ty-y^TX\theta +y^Ty \right)\\\end{aligned}</script><p>接下来对 J(θ) 求偏导，需要用到<strong>矩阵的求导法则</strong>（证明过程略去不表）：</p><blockquote><ol><li>当 $f(x) = Ax$ 时，<script type="math/tex; mode=display">\frac{\partial f (x)}{\partial x^T}  = \frac{\partial (Ax)}{\partial x^T}  =A</script></li></ol><ol><li><p>当$f(x) = x^TAx$ 时，</p><script type="math/tex; mode=display">\frac{\partial f (x)}{\partial x}  = \frac{\partial (x^TAx)}{\partial x}  =Ax+A^Tx</script></li><li><p>当 $f(x) = a^Tx$ 时，</p><script type="math/tex; mode=display">\frac{\partial a^Tx}{\partial x}  = \frac{\partial x^Ta}{\partial x}  =a</script></li><li><p>当 $f(x) = x^TAy$时，</p><script type="math/tex; mode=display">\frac{\partial x^TAy}{\partial x}  = Ay</script></li></ol></blockquote><p>分别用法则 2、4、3 求导，得到：</p><script type="math/tex; mode=display">\begin{aligned}    \frac{\partial J\left( \theta \right)}{\partial \theta}&=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-(y^TX)^T+0 \right)\\    &=\frac{1}{2m}\left( 2X^TX\theta -X^Ty-X^Ty+0 \right)\\    &=\frac{1}{m}\left(X^TX\theta -X^Ty\right)\\\end{aligned}</script><p>令偏导为零，解得：</p><script type="math/tex; mode=display">\theta =\left( X^TX \right) ^{-1}X^Ty</script><h3 id="梯度下降-vs-正规方程"><a href="#梯度下降-vs-正规方程" class="headerlink" title="梯度下降 vs. 正规方程"></a>梯度下降 vs. 正规方程</h3><p>观察到在正规方程的结果中，$X^TX$ 是一个 $(n+1)\times(n+1)$ 的矩阵，因此直接取逆计算 $θ$ 的复杂度是 $O(n^3)$ 。如果 $n$ 不是很大，这是有效的，但是如果 $n$ 达到了 $10^4,10^5$ 或更高，就需要使用梯度下降了。</p><p>下面从其他方面对两种算法进行比较：</p><div class="table-container"><table><thead><tr><th style="text-align:center">区别</th><th style="text-align:center">梯度下降</th><th style="text-align:center">正规方程</th></tr></thead><tbody><tr><td style="text-align:center">学习率 α</td><td style="text-align:center">需要选择</td><td style="text-align:center">不需要</td></tr><tr><td style="text-align:center">迭代</td><td style="text-align:center">需要多次迭代</td><td style="text-align:center">一次运算得出</td></tr><tr><td style="text-align:center">n 的取值</td><td style="text-align:center">当 n 大时也能较好适用</td><td style="text-align:center">当 n 小于时 104 还是可以接受的</td></tr><tr><td style="text-align:center">特征缩放</td><td style="text-align:center">特征取值范围相差大时需要</td><td style="text-align:center">不需要缩放</td></tr><tr><td style="text-align:center">适用情形</td><td style="text-align:center">适用于各种类型的模型</td><td style="text-align:center">只适用于线性模型</td></tr></tbody></table></div><blockquote><p>这里提及适用情形，是因为随着问题的深入，算法将越发复杂。例如在分类算法中的逻辑回归等模型，就无法使用正规方程求解。</p></blockquote><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面仍以 <code>ex1data2.txt</code> 为例实现：</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs stan">import numpy as np<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br><span class="hljs-title">data</span> = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=&#x27;,&#x27;)<br>(m, n) = <span class="hljs-title">data</span>.shape<br>X = np.c_[np.ones(m), <span class="hljs-title">data</span>[:, :-<span class="hljs-number">1</span>]]<br>y = <span class="hljs-title">data</span>[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Normal Equation</span><br>theta = np.linalg.<span class="hljs-built_in">inv</span>(X.T @ X) @ X.T @ y<br><span class="hljs-built_in">print</span>(theta)<br><br><span class="hljs-comment"># predict</span><br>predict = np.<span class="hljs-type">array</span>([<span class="hljs-number">1</span>, <span class="hljs-number">1650</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br>PYTHON<br></code></pre></td></tr></table></figure><p>很快就计算完了，预测在 $\left( x_1=1650,x_2=3 \right)$ 时的房价为 293081.46433489426。大约相当于 3000 次梯度下降迭代的精度。</p><h3 id="不可逆情形"><a href="#不可逆情形" class="headerlink" title="不可逆情形"></a>不可逆情形</h3><p>前一节的推导基于 $X^TX$ 可逆（Invertible）的假设，如若不可逆（Non-invertible，也称 Singular），我们只需将代码中的 <code>inv()</code> 换成 <code>pinv()</code> 求出<strong>伪逆矩阵</strong>即可。</p><p>通常导致矩阵不可逆的原因可能有：</p><ul><li>存在冗余特征（特征之间不相互独立）；</li><li>特征数 $n$ 远大于样本数 $m$（样本数不足以刻画这么多特征）。</li></ul><p>解决方法对应为：</p><ul><li>删除冗余特征（线性相关特征只保留其一）；</li><li>削减非必要的特征，或<strong>正则化</strong>方法（Regularization），后文将介绍。</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#2 梯度下降：多元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<p>在前文 <a href="http://hexiaobo.xyz/ML学习笔记-1-梯度下降：一元线性回归">一元线性回归</a> 的基础上，我们引入多个特征变量，探讨梯度下降对多元线性回归的解法。此外，下一节将介绍正规方程在解多元线性回归中的应用。</p><h2 id="多元线性回归-Multiple-Linear-Regression"><a href="#多元线性回归-Multiple-Linear-Regression" class="headerlink" title="多元线性回归 | Multiple Linear Regression"></a>多元线性回归 | Multiple Linear Regression</h2><p>现在我们的样本点 $\left(x^{(i)}, y^{(i)}\right)$ 有多个特征作为<strong>输入变量</strong>，即给定的数据集为：</p><script type="math/tex; mode=display">\left\{\left(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}</script><ul><li>$n$ 代表单个样本的特征数量；</li><li>${x}^{(i)}$ 代表第 i 个观察实例的<strong>特征向量</strong>；</li><li>$x^{(i)}_j$ 代表第 i 个观察实例的第$ j $个<strong>特征分量</strong>。</li></ul><p>同时，回归方程 h 也具有多个参数$\theta_0,\theta_1,\cdots,\theta_n$：</p><script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x_1\cdots+\theta_nx_n</script><p>为简化表达式，这里假定$x_0 \equiv 1$ ，并以<strong>向量</strong>（vector）表示参数和自变量：$\theta=(\theta_0,\cdots,\theta_n)^T,\;x=(x_0,\cdots,x_n)^T$，得到：</p><script type="math/tex; mode=display">h_\theta(x)=\theta^Tx</script><h3 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h3><p>类似地，我们定义平方误差代价函数：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2</script><p>我们的目标和一元线性回归中一样，要找出使得代价函数最小的一系列参数。于是，</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}</script><p>梯度下降时，不断作迭代：</p><script type="math/tex; mode=display">\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}</script><p>即可。</p><h3 id="特征缩放与标准化-Standardization"><a href="#特征缩放与标准化-Standardization" class="headerlink" title="特征缩放与标准化 | Standardization"></a>特征缩放与标准化 | Standardization</h3><p>当不同自变量取值范围相差较大时，绘制的<strong>等高线图</strong>上的椭圆会变得瘦长，而梯度下降<strong>算法收敛</strong>将会很慢，因为每一步都可能会跨过这个椭圆导致<strong>振荡</strong>。这里略去数学上的证明。同理，所有依赖于「<strong>距离计算</strong>」的机器学习算法也会有此问题。</p><p>此时，我们需要把所有<strong>自变量</strong>（除了假定的 x0）进行缩放、标准化，使其落在 -1 到 1 之间。最简单的方法是，置：</p><script type="math/tex; mode=display">x_i^{(j)}:=\frac{x_i^{(j)}-\mu_i}{\sigma_i}</script><p>其中，$\mu_i=\frac{1}{m}\sum\limits_{j=1}^m x_i^{(j)}$是样本<strong>均值</strong>（Mean Value），$\sigma_i=\sqrt{\frac{\sum\limits_{j=1}^m\left(x_i^{(j)}-\mu_i\right)^2}{m-1}}$是样本<strong>无偏标准差</strong>（Unbiased Standerd Deviation），就完成了<strong>标准化</strong>（Standardization）。标准化后样本均值为 0，方差为 1，但不一定是标准正态分布（与其原始分布有关），根据中心极限定理可以推出。</p><p>需要注意的是，<strong>因变量</strong>不需要标准化，否则计算的结果将失真。且如果进行了标准化，对所有<strong>待测</strong>样本点也需要进行一样的操作，参数才能生效。</p><blockquote><p>此外，线性回归并不适用于所有情形，有时我们需要曲线来适应我们的数据，这时候我们也要对特征进行<strong>构造</strong>，如二次函数、三次函数、幂函数、对数函数等。构造后的新变量就可以当作一个新的特征来使用，这就是<strong>多项式回归</strong>（Polynomial Regression）。新变量的取值范围可能更大，此时，特征缩放就非常有必要！</p></blockquote><h3 id="归一化-Normalization"><a href="#归一化-Normalization" class="headerlink" title="归一化 | Normalization"></a>归一化 | Normalization</h3><p>人们经常会混淆标准化（Standardization）与<strong>归一化</strong>（Normalization）的概念，这里也简单提一下：归一化的目的是找到某种映射关系，将原数据<strong>固定映射</strong>到某个区间 $[a,b]$上，而标准化则没有限制。</p><p>归一化最常用于把有量纲数转化为<strong>无量纲数</strong>，让不同维度之间的特征在数值上有一定比较性，比如 Min-Max Normalization：</p><script type="math/tex; mode=display">x_{i}^{(j)}:=\frac{x_{i}^{(j)}-\min \left( x_i \right)}{\max \left( x_i \right) -\min \left( x_i \right)}</script><p>但是，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。因为「<strong>仅由极值决定</strong>」这个做法过于危险，如果样本中有一个异常大的值，则会将所有正常值挤占到很小的区间，而标准化方法则更加「弹性」，会兼顾所有样本。</p><h3 id="学习率-α"><a href="#学习率-α" class="headerlink" title="学习率 α"></a>学习率 α</h3><p>上一节谈到，学习率（Learnig rate）的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。通过绘制<strong>迭代收敛曲线</strong>（Convergence Graph）可以看出学习率的好坏，也可以看出何时算法能收敛并及时<strong>终止算法</strong>。</p><p><a href="http://img.hexiaobo.xyz/cost-iter.png"><img src="http://img.hexiaobo.xyz/cost-iter.png" alt="代价函数-迭代次数"></a></p><p>代价函数-迭代次数</p><p>特别地，当 $\alpha$ 取值过大时，曲线可能呈现<strong>上扬</strong>或<strong>波浪线</strong>型，解决办法都是选择更小的 α 值。可以证明，只要 $\alpha$足够小，凸函数都会收敛于极点。</p><p>此外，还有一种终止算法的方法：判断在某次或连续$ n $次迭代后 $J(θ)$的变化小于某个极小量，如$\varepsilon =1e^{-3}$，此时就可以认为算法终止。但这种办法则不能用于选择尽量大的 $α$ 值。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面以 <code>ex1data2.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (47, 3)</span><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = data[:, :-<span class="hljs-number">1</span>]<br>y = data[:, -<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># normalization</span><br>mu = X.mean(axis=<span class="hljs-number">0</span>)<br>sigma = X.std(axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br>X = (X - mu) / sigma<br>X = np.c_[np.ones(m), X] <span class="hljs-comment"># 增加一列 1</span><br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">1500</span><br>theta = np.zeros(n)<br>J_history = np.zeros(num_iters)<br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (47, )</span><br>theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>)<br>J_history[i] = np.<span class="hljs-built_in">sum</span>(np.power(error, <span class="hljs-number">2</span>)) / (<span class="hljs-number">2</span> * m)<br><br><span class="hljs-comment"># predict</span><br>predict = (np.array([<span class="hljs-number">1650</span>, <span class="hljs-number">3</span>]) - mu) / sigma<br>predict = np.r_[<span class="hljs-number">1</span>, predict]<br><span class="hljs-built_in">print</span>(predict @ theta)<br><br><span class="hljs-comment"># plot the convergence graph</span><br>plt.plot(np.arange(J_history.size), J_history)<br>plt.xlabel(<span class="hljs-string">&#x27;Number of iterations&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Cost J&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的$\left( \theta_0, \theta_1, \theta_2 \right)$结果是：[340412.6595 110631.0484 -6649.4724]，预测在$\left( x_1=1650,x_2=3 \right)$时的房价为 293101.0568574823。</p><p>绘制的迭代收敛曲线如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><img src="http://img.hexiaobo.xyz/Figure_1.png" alt="多元线性回归的迭代收敛曲线"></a></p><p>多元线性回归的迭代收敛曲线</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记#1 梯度下降：一元线性回归</title>
    <link href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html"/>
    <url>/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.html</url>
    
    <content type="html"><![CDATA[<h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><p>首先给出两个「机器学习」的定义，尽管对于相关从业者来说，也不存在一个被广泛认可的定义来准确定义机器学习是什么，但或许有助于回答诸如「机器学习是什么？」之类的面试问题：</p><ul><li>Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn <strong>without</strong> being explicitly programmed.</li><li>Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to <em>learn</em> from <strong>experience</strong> E with respect to some <strong>task</strong> T and some <strong>performance measure</strong> P, if its performance on T, as measured by P, improves with experience E.</li></ul><h3 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习 | Supervised Learning"></a>监督学习 | Supervised Learning</h3><p>如果我们给学习算法一个数据集，这个数据集由「<strong>正确答案</strong>」组成，然后运用该算法算出更多可能正确的答案，这就是监督学习。</p><p>监督学习领域有两大问题：</p><ul><li>回归（Regression）：以一系列<strong>离散值</strong>（discrete），试着推测出这一系列<strong>连续值</strong>（continuous）属性。譬如根据已知房价预测未知房价。</li><li>分类（Classification）：以一系列<strong>离散值</strong>，试着推测出同样是<strong>离散值</strong>的属性。譬如预测肿瘤的恶性与否。</li></ul><p>对于分类问题，输出的结果可以大于两个（分出许多类），输入的<strong>特征</strong>（feature）也可以更多，例如通过患者年龄和肿瘤大小等特征预测肿瘤的恶性与否。</p><p>在绘制这些样本时，可以用一个<strong>多维空间</strong>中不同颜色的<strong>样本点</strong>来表示。在一些问题中，我们希望使用无穷多的特征来学习，显然电脑的内存肯定不够用，而后文将介绍的<strong>支持向量机</strong>（SVM，Support Vector Machine）巧妙地解决了这个问题。</p><h3 id="无监督学习-Unsupervised-Learning"><a href="#无监督学习-Unsupervised-Learning" class="headerlink" title="无监督学习 | Unsupervised Learning"></a>无监督学习 | Unsupervised Learning</h3><p>对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案。而在无监督学习中，这些数据「<strong>没有任何标签</strong>」或是只有「<strong>相同的标签</strong>」。</p><p>因此，我们希望学习算法能够判断出数据具有不同的<strong>聚集簇</strong>（Cluster），这就是无监督学习领域的经典问题：<strong>聚类算</strong>（Cluster Algorithm）。</p><p>譬如在谷歌新闻中，我们对收集的网络新闻分组，组成有关联的新闻；又如在鸡尾酒宴会问题中，两个位置不同的麦克风录到的两个音源，通过学习算法可以各自分离出来。</p><h2 id="一元线性回归-Univariate-Linear-Regression"><a href="#一元线性回归-Univariate-Linear-Regression" class="headerlink" title="一元线性回归 | Univariate Linear Regression"></a>一元线性回归 | Univariate Linear Regression</h2><p>前文提到，回归是监督学习的一个经典问题，这里我们将以线性回归来一窥整个监督学习的模型建立过程。假设已经拥有了回归问题的<strong>数据集</strong>，我们将之描述为：</p><script type="math/tex; mode=display">{(x(i),y(i)),i=1,2,⋯,m}</script><ul><li>$m$ 代表训练集中实例的数量；</li><li>$x $代表<strong>特征</strong> or 输入变量；</li><li>$y $代表<strong>目标变量</strong> or 输出变量；</li><li>$({x}^{(i)},{y}^{(i)})$ 代表第 i 个观察实例；</li><li>$h$ 代表学习算法的解决方案或函数也称为<strong>假设</strong>（hypothesis）。</li></ul><p>为了解决这样一个问题，我们实际上是要将训练集「喂」给我们的学习算法，进而学习得到一个假设 $h$。在本文中，我们尝试用线性函数 $h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$ 拟合之。因为只含有一个特征 or 输入变量，因此这样的问题叫作<strong>一元线性回归</strong>问题。</p><h3 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 | Cost Function"></a>代价函数 | Cost Function</h3><p>有了上文建立的基础模型，现在要做的便是为之选择合适的<strong>参数</strong>（parameters）$\theta_{0}$ 和 $\theta_{1}$，即直线的斜率和在 $y$ 轴上的截距。</p><p>选择的参数决定了得到的直线相对于训练集的<strong>准确程度</strong>，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>（modeling error）。我们要做的就是尽量选择参数使得误差降低，即最小化 $h_{\theta}(x^{(i)})$ 和 $h_{\theta}(y^{(i)})$ 的距离。于是我们有了经典的<strong>平方误差代价函数</strong>：</p><script type="math/tex; mode=display">J\left( \theta _0,\theta _1 \right) =\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}(x^{(i)})-y^{(i)} \right) ^2}</script><p>也即是每个数据纵坐标的预测值与真实值的差的平方之和的平均，除以 2 仅是为了后续求导方便，没有什么本质的影响。</p><p>绘制一个三维空间，三个坐标分别为 $\theta _0$ 和 $\theta _1$ 和 $J(\theta _0,\theta _1)$，对每个 $(\theta _0,\theta _1)$ 对，代入训练集可以得到一个 $J(\theta _0,\theta _1)$值，经过一系列计算，我们得到一个<strong>碗状曲面</strong>：</p><p><a href="http://img.hexiaobo.xyz/3d-surface.png"><img src="http://img.hexiaobo.xyz/3d-surface.png" alt="三维空间中的碗状曲面"></a></p><p>三维空间中的碗状曲面</p><p>显然，在三维空间中存在一个使得 $J(\theta_{0}, \theta_{1})$最小的点。为了更好地表达，我们将三维空间投影到二维，得到<strong>等高线图</strong>（Contour plot）：</p><p><a href="http://img.hexiaobo.xyz/contour.png"><img src="http://img.hexiaobo.xyz/contour.png" alt="二维空间的等高线图"></a></p><p>二维空间的等高线图</p><p>这些同心椭圆的中心，就是我们要找到使得 $J(\theta_{0}, \theta_{1})$ 最小的点。</p><h3 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法 | Gradient Descent"></a>梯度下降法 | Gradient Descent</h3><p>在数学上，我们知道最小二乘法可以解决一元线性回归问题。现在有了等高线图，我们需要的是一种有效的算法，能够自动地找出这些使代价函数$ J $取最小值的 <script type="math/tex">\left( \theta_0, \theta_1 \right)</script>对。当然并不是把这些点画出来，然后人工读出中心点的数值——对于更复杂、更高维度、更多参数的情况，我们很难将其可视化。</p><p>梯度下降是一个用来求<strong>一般函数最小值</strong>的算法，其背后的思想是：开始时<strong>随机选择</strong>一个参数组合$\left( \theta_{0},\theta_{1},……,\theta_{n} \right)$，计算代价函数，然后一点点地修改参数，直到找到<strong>下一个</strong>能让代价函数值下降最多的参数组合。</p><p>持续这么做会让我们找到一个<strong>局部最小值</strong>（local minimum），但我们无法确定其是否就是<strong>全局最小值</strong>（global minimum），哪怕只是稍微修改初始参数，也可能最终结果完全不同。当然，在一元线性回归问题中，生成的曲面始终是<strong>凸函数</strong>（convex function），因此我们可以不考虑这个问题。</p><p>在高等数学中，我们知道「下降最多」其实指的是函数 J 的<strong>梯度方向的逆方向</strong>，也即方向导数最大的方向，梯度定义为：</p><script type="math/tex; mode=display">\nabla J=\mathrm{grad} J=\left\{ \frac{\partial J}{\partial \theta _0},\frac{\partial J}{\partial \theta _1} \right\}</script><p>所以不断迭代进行赋值：</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\cdot\frac{\partial}{\partial\theta_j} J(\theta_{0}, \theta_{1})</script><p>直到收敛，即可找到一个<strong>极小值</strong>。其中，α 就是这一步的<strong>步长</strong>，也称为<strong>学习率</strong>（Learnig rate）。学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>对代价函数求偏导：</p><ul><li>$j=0$ 时：$\frac{\partial}{\partial \theta _0}J(\theta _0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)} \right)}$</li><li>$j=1$ 时：$\frac{\partial}{\partial \theta _1}J(\theta _0,\theta _1)=\frac{1}{m}\sum_{i=1}^m{x^{(i)}\left( \theta _1x^{(i)}+\theta _0-y^{(i)} \right)}$</li></ul><p>所以梯度方向为：</p><script type="math/tex; mode=display">\mathrm{grad } J=\left\{ \frac{\partial J}{\partial \theta _0},\frac{\partial J}{\partial \theta _1} \right\} =\left\{ \frac{1}{m}\sum_{i=1}^m{\left( \theta _1x^{(i)}+\theta _0-y^{(i)} \right)},\frac{1}{m}\sum_{i=1}^mx^{(i)}\left( \theta _1x^{(i)}+\theta _0-y^{(i)} \right) \right\}</script><p>此外，我们注意到在<strong>每一步</strong>迭代过程中，我们都用到了<strong>所有的</strong>训练样本，如 $\sum_{i=1}^mx^{(i)}$、$\sum_{i=1}^my^{(i)}$、$\sum_{i=1}^mx^{(i)}y^{(i)}$等项，这也称为<strong>批量梯度下降</strong>（Batch Gradient Descent）。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>需要注意的是，迭代赋值的过程中，$\left( \theta_0, \theta_1 \right)$的值要同时更新，否则就会与梯度方向有微小区别。用 Python 元组解包赋值可以实现，用矩阵乘法也可实现。</p><p>为了用向量化加速计算，这里利用了一个小性质，就是当$a$和 $b$都为向量时有 $a^Tb=b^Ta$，所以有：</p><script type="math/tex; mode=display">X\theta =\left[ \begin{array}{c}    -\left( x^{(1)} \right) ^T\theta -\\    -\left( x^{(2)} \right) ^T\theta -\\    \vdots\\    -\left( x^{(m)} \right) ^T\theta -\\\end{array} \right] =\left[ \begin{array}{c}    -\theta ^T\left( x^{(1)} \right) -\\    -\theta ^T\left( x^{(2)} \right) -\\    \vdots\\    -\theta ^T\left( x^{(m)} \right) -\\\end{array} \right]</script><p>下面以一元线性回归数据集 <code>ex1data1.txt</code> 为例实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># load data, data.shape = (97, 2)</span><br>data = np.loadtxt(<span class="hljs-string">&#x27;ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>, usecols=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>x = data[:, <span class="hljs-number">0</span>]<br>y = data[:, <span class="hljs-number">1</span>]<br>m = y.size<br><br><span class="hljs-comment"># parameters</span><br>alpha = <span class="hljs-number">0.01</span><br>num_iters = <span class="hljs-number">5000</span><br>theta = np.zeros(<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># X.shape = (97, 2), y.shape = (97, ), theta.shape = (2, )</span><br>X = np.c_[np.ones(m), x]  <span class="hljs-comment"># 增加一列 1 到 矩阵 X，实现多项式运算</span><br><br><span class="hljs-comment"># Gradient Descent</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_iters):<br>    error = (X @ theta).flatten() - y  <span class="hljs-comment"># error.shape = (97, )</span><br>    theta -= (alpha / m) * np.<span class="hljs-built_in">sum</span>(X * error[:, np.newaxis], <span class="hljs-number">0</span>) <span class="hljs-comment"># 0 表示每列相加</span><br><br><span class="hljs-comment"># plot</span><br><span class="hljs-built_in">print</span>(theta)<br>plt.scatter(x, y)<br>x_plot = np.array([np.<span class="hljs-built_in">min</span>(x), np.<span class="hljs-built_in">max</span>(x)])<br>y_plot = theta[<span class="hljs-number">0</span>] + x_plot * theta[<span class="hljs-number">1</span>]  <span class="hljs-comment"># NumPy Broadcast</span><br>plt.plot(x_plot, y_plot, c=<span class="hljs-string">&quot;m&quot;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><p>得到的$\left( \theta_0, \theta_1 \right)$ 结果是：[-3.8953 1.1929]，作图如下：</p><p><a href="http://img.hexiaobo.xyz/Figure_1.png"><img src="http://img.hexiaobo.xyz/Figure_1.png" alt="一元线性回归"></a></p><p>一元线性回归</p><blockquote><p><strong>向量化</strong>：在 Python 或 Matlab 中进行科学计算时，如果有多组数据或多项式运算，转化成矩阵乘法会比直接用 <code>for</code> 循环高效很多，可以实现<strong>同时赋值</strong>且代码更简洁。</p><p>这是由于科学计算库中的函数更好地利用了硬件的特性，更好地支持了诸如<strong>循环展开、流水线、超标量</strong>等技术。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
