<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="He Xiaobo"><meta name="keywords" content=""><meta name="description" content="特征选择学习记录一"><meta property="og:type" content="article"><meta property="og:title" content="特征选择（Feature selection）方法汇总"><meta property="og:url" content="http://hexiaobo.xyz/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-selection%EF%BC%89%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB"><meta property="og:site_name" content="He xiaobo"><meta property="og:description" content="特征选择学习记录一"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://hexiaobo.xyz/img/fs.jpg"><meta property="article:published_time" content="2022-11-14T07:08:27.000Z"><meta property="article:modified_time" content="2022-11-14T07:20:22.203Z"><meta property="article:author" content="He Xiaobo"><meta property="article:tag" content="特征选择（Feature selection）"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://hexiaobo.xyz/img/fs.jpg"><meta name="referrer" content="no-referrer-when-downgrade"><title>特征选择（Feature selection）方法汇总 - He xiaobo</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"hexiaobo.xyz",root:"/",version:"1.9.3",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>xiaobo.He</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="特征选择（Feature selection）方法汇总"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-11-14 15:08" pubdate>2022年11月14日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 7.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 64 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">特征选择（Feature selection）方法汇总</h1><p class="note note-info">本文最后更新于：2022年11月14日 下午</p><div class="markdown-body"><h2 id="介绍">介绍</h2><p><code>特征选择</code>是<code>特征工程</code>里的一个重要问题，其目标是<strong>寻找最优特征子集</strong>。特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，<strong>提高模型精确度，减少运行时间的目的</strong>。另一方面，选取出真正相关的特征简化模型，协助理解数据产生的过程。并且常能听到“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”，由此可见其重要性。但是它几乎很少出现于机器学习书本里面的某一章。然而在机器学习方面的成功很大程度上在于如果使用特征工程。</p><p>之所以要考虑特征选择，是因为机器学习经常面临过拟合的问题。 <strong>过拟合</strong>的表现是模型参数<strong>太贴合训练集数据</strong>，模型在训练集上效果很好而在测试集上表现不好，也就是在高方差。简言之模型的泛化能力差。过拟合的原因是模型对于训练集数据来说太复杂，要解决过拟合问题，一般考虑如下方法：</p><ol type="1"><li>收集更多数据</li><li>通过正则化引入对复杂度的惩罚</li><li>选择更少参数的简单模型</li><li>对数据降维（降维有两种方式：特征选择和特征抽取）</li></ol><p>其中第1条一般是很难做到的，一般主要采用第2和第4点</p><h2 id="一般流程">一般流程</h2><p>特征选择的一般过程：</p><ol type="1"><li>生成子集：搜索特征子集，为评价函数提供特征子集</li><li>评价函数：评价特征子集的好坏</li><li>停止准则：与评价函数相关，一般是阈值，评价函数达到一定标准后就可停止搜索</li><li>验证过程：在验证数据集上验证选出来的特征子集的有效性</li></ol><p>但是， 当特征数量很大的时候， 这个搜索空间会很大，如何找最优特征还是需要一些经验结论。</p><h2 id="三大类方法">三大类方法</h2><p>根据特征选择的形式，可分为三大类：</p><ul><li>Filter(过滤法)：按照<code>发散性</code>或<code>相关性</code>对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选</li><li>Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征</li><li>Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）</li></ul><h2 id="过滤法">过滤法</h2><p>基本想法是：分别对每个特征 <span class="math inline">\(x_i\)</span> ，计算 <span class="math inline">\(x_i\)</span> 相对于类别标签 y 的信息量 S(i) ，得到 n 个结果。然后将 n 个 S(i) 按照从大到小排序，输出前 k 个特征。显然，这样复杂度大大降低。那么关键的问题就是使用什么样的方法来度量 S(i) ，我们的目标是选取与 y 关联最密切的一些 特征<span class="math inline">\(x_i\)</span> 。</p><ul><li>Pearson相关系数</li><li>卡方验证</li><li>互信息和最大信息系数</li><li>距离相关系数</li><li>方差选择法</li></ul><h3 id="pearson相关系数">Pearson相关系数</h3><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，衡量的是变量之间的线性相关性，结果的取值区间为<strong>[-1,1]</strong> ， -1 表示完全的负相关(这个变量下降，那个就会上升)， +1 表示完全的正相关， 0 表示没有线性相关性。Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的<a href="https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html">pearsonr</a>方法能够同时计算相关系数和p-value</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> pearsonr<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>size = <span class="hljs-number">300</span><br>x = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Lower noise：&quot;</span>, pearsonr(x, x + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Higher noise：&quot;</span>, pearsonr(x, x + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, size)))<br><br><br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-comment"># 选择K个最好的特征，返回选择特征后的数据</span><br><span class="hljs-comment"># 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数</span><br><span class="hljs-comment"># 参数k为选择的特征个数</span><br>SelectKBest(<span class="hljs-keyword">lambda</span> X, Y: array(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><p>Pearson相关系数的一个<strong>明显缺陷</strong>是，作为特征排序机制，他<strong>只对线性关系敏感</strong>。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近 0 。</p><h3 id="卡方验证">卡方验证</h3><p>经典的卡方检验是检验<strong>类别型变量</strong>对<strong>类别型变量</strong>的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：</p><p><span class="math display">\[χ2=∑(A−E)2E\]</span></p><p>不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用sklearn中feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> chi2<br>iris = load_iris()<br>X, y = iris.data, iris.target  <span class="hljs-comment">#iris数据集</span><br><br><span class="hljs-comment">#选择K个最好的特征，返回选择特征后的数据</span><br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><p><a href="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/classes.html%23module-sklearn.feature_selection">sklearn.feature_selection</a>模块中的类可以用于样本集中的特征选择/维数降低，以提高估计器的准确度分数或提高其在非常高维数据集上的性能</p><h3 id="互信息和最大信息系数-mutual-information-and-maximal-information-coefficient-mic">互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)</h3><p>经典的互信息也是评价<strong>类别型变量</strong>对<strong>类别型变量</strong>的相关性的，互信息公式如下：</p><p><span class="math display">\[MI(x_i,y)=∑x_i∈0,1∑y∈0,1 \ p(x_i,y)logp(x_i,y)p(x_i)p(y)\]</span></p><p>当<span class="math inline">\(x_i\)</span>是0/1离散值的时候，这个公式如上。很容易推广到 <span class="math inline">\(x_i\)</span> 是多个离散值的情况。这里的 <span class="math inline">\(p(x_i,y) , p(x_i)\)</span> 和 p(y) 都是从训练集上得到的。若问这个 MI 公式如何得来，请看它的 KL 距离（Kullback-Leibler）表述： $MI(x_i,y)=KL(P(x_i,y)||p(x_i)p(y)) $也就是说, MI 衡量的是 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(y\)</span> 的独立性。如果它俩独立 <span class="math inline">\(P(x_i,y)=p(x_i)p(y)\)</span> ，那么 KL 距离值为0，也就是 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(y\)</span> 不相关了，可以去除 <span class="math inline">\(x_i\)</span>。相反，如果两者密切相关，那么 MI 值会很大。在对 MI 进行排名后，最后剩余的问题就是如何选择 k 个值（前 k 个 <span class="math inline">\(x_i\)</span> ）。(后面将会提到此方法)我们继续使用交叉验证的方法，将 k 从 1 扫描到 n ，取评分最高的k 。 不过这次复杂度是线性的了。比如，在使用朴素贝叶斯分类文本的时候，词表长度 n 很大。 使用filiter特征选择方法，能够增加分类器精度。</p><p>想把互信息直接用于特征选择其实不是太方便：</p><ol type="1"><li>它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较</li><li>对于连续变量的计算不是很方便（ X 和 Y 都是集合, <span class="math inline">\(x_i\)</span>, <span class="math inline">\(y\)</span> 都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感</li></ol><p><strong>最大信息系数</strong>克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在 [0,1] 。<a href="https://link.zhihu.com/?target=https%3A//minepy.readthedocs.io/en/latest/">minepy</a>提供了MIC功能。</p><p>下面我们来看下 <span class="math inline">\(y=x^2\)</span> 这个例子，MIC算出来的互信息值为1(最大的取值)。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> minepy <span class="hljs-keyword">import</span> MINE<br><br>m = MINE()<br>x = np.random.uniform(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10000</span>)<br>m.compute_score(x, x**<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(m.mic())<br><br><br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-comment">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mic</span>(<span class="hljs-params">x, y</span>):<br>    m = MINE()<br>    m.compute_score(x, y)<br>    <span class="hljs-keyword">return</span> (m.mic(), <span class="hljs-number">0.5</span>)<br><span class="hljs-comment"># 选择K个最好的特征，返回特征选择后的数据</span><br>SelectKBest(<span class="hljs-keyword">lambda</span> X, Y: array(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><h3 id="距离相关系数">距离相关系数</h3><p>距离相关系数是为了克服Pearson相关系数的弱点而生的。在 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(x^2\)</span> 这个例子中，即便Pearson相关系数是 0 ，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是 0 ，那么我们就可以说这两个变量是独立的。</p><p>R的<a href="https://link.zhihu.com/?target=https%3A//cran.r-project.org/web/packages/energy/index.html">energy</a>包里提供了距离相关系数的实现，另外这是<a href="https://link.zhihu.com/?target=https%3A//gist.github.com/josef-pkt/2938402">Python gist</a>的实现。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text"># R-code<br>&gt; x = runif (1000, -1, 1)<br>&gt; dcor(x, x**2)<br>[1] 0.4943864<br></code></pre></td></tr></table></figure><p>尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。</p><p>第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。</p><p>第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。</p><h3 id="方差选择法">方差选择法</h3><p>过滤特征选择法还有一种方法不需要度量特征 <span class="math inline">\(x_i\)</span> 和类别标签 <span class="math inline">\(y\)</span> 的信息量。这种方法先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p><p>例如，假设我们有一个具有布尔特征的数据集，并且我们要删除超过80％的样本中的一个或零（开或关）的所有特征。布尔特征是伯努利随机变量，这些变量的方差由下式给出: <span class="math inline">\(Var[X]=p(1−p)\)</span></p><p><a href="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html%23sklearn.feature_selection.VarianceThreshold">VarianceThreshold</a>是特征选择的简单基线方法。它删除方差不符合某个阈值的所有特征。默认情况下，它会删除所有零差异特征，即所有样本中具有相同值的特征。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br>X = [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]<br><span class="hljs-comment"># 方差选择法，返回值为特征选择后的数据</span><br><span class="hljs-comment"># 参数threshold为方差的阈值</span><br>sel = VarianceThreshold(threshold=(<span class="hljs-number">.8</span> * (<span class="hljs-number">1</span> - <span class="hljs-number">.8</span>)))<br><span class="hljs-built_in">print</span>(sel.fit_transform(X))<br><br><span class="hljs-comment"># VarianceThreshold(threshold=3).fit_transform(iris.data)</span><br></code></pre></td></tr></table></figure><p>输出结果：</p><p>array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]]) 如预期的那样，VarianceThreshold已经删除了第一列，其具有 <span class="math inline">\(p=5/6&gt;0.8\)</span> 包含零的概率。</p><p>方差选择的逻辑并不是很合理，这个是基于各特征分布较为接近的时候，才能以方差的逻辑来衡量信息量。但是如果是离散的或是仅集中在几个数值上，如果分布过于集中，其信息量则较小。而对于连续变量，由于阈值可以连续变化，所以信息量不随方差而变。 实际使用时，可以结合cross-validate进行检验</p><h2 id="包装法">包装法</h2><p>基本思想：基于hold-out方法，对于每一个待选的特征子集，都在训练集上训练一遍模型，然后在测试集上根据误差大小选择出特征子集。需要先选定特定算法，通常选用普遍效果较好的算法， 例如Random Forest， SVM， kNN等等。</p><blockquote><p>西瓜书上说包装法应该欲训练什么算法，就选择该算法进行评估 随着学习器（评估器）的改变，最佳特征组合可能会改变</p></blockquote><p>贪婪搜索算法（greedy search）是局部最优算法。与之对应的是穷举算法 (exhaustive search)，穷举算法是遍历所有可能的组合达到全局最优级，但是计算复杂度是2^n，一般是不太实际的算法。</p><h3 id="前向搜索">前向搜索</h3><p>前向搜索说白了就是，每次增量地从剩余未选中的特征选出一个加入特征集中，待达到阈值或者 n 时，从所有的 F 中选出错误率最小的。过程如下：</p><ol type="1"><li>初始化特征集 F 为空。</li><li>扫描 i 从 1 到 n 如果第 i 个特征不在 <span class="math inline">\(F\)</span> 中，那么特征 i 和F 放在一起作为 <span class="math inline">\(F_i\)</span> (即$ F_i=F )$。 在只使用 <span class="math inline">\(F_i\)</span>中特征的情况下，利用交叉验证来得到 F_i 的错误率。</li><li>从上步中得到的 n 个 <span class="math inline">\(F_i\)</span> 中选出错误率最小的 <span class="math inline">\(F_i\)</span> ,更新 F 为 <span class="math inline">\(F_i\)</span>。</li><li>如果 F 中的特征数达到了 n 或者预定的阈值（如果有的话）， 那么输出整个搜索过程中最好的 ；若没达到，则转到 2，继续扫描。</li></ol><h3 id="后向搜索">后向搜索</h3><p>既然有增量加，那么也会有增量减，后者称为后向搜索。先将 F 设置为 <span class="math inline">\({1,2,...,n}\)</span> ，然后每次删除一个特征，并评价，直到达到阈值或者为空，然后选择最佳的 F 。</p><p>这两种算法都可以工作，但是计算复杂度比较大。时间复杂度为：O(n+(n−1)+(n−2)+...+1)=O(n2)</p><h3 id="递归特征消除法">递归特征消除法</h3><p>递归消除特征法使用一个<code>基模型</code>来进行多轮训练，每轮训练后通过学习器返回的 coef_ 或者feature_importances_ 消除若干权重较低的特征，再基于新的特征集进行下一轮训练。</p><p>使用feature_selection库的RFE类来选择特征的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br><span class="hljs-comment">#递归特征消除法，返回特征选择后的数据</span><br><span class="hljs-comment">#参数estimator为基模型</span><br><span class="hljs-comment">#参数n_features_to_select为选择的特征个数</span><br>RFE(estimator=LogisticRegression(), n_features_to_select=<span class="hljs-number">2</span>).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><h2 id="嵌入法">嵌入法</h2><ul><li>基于惩罚项的特征选择法 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br><span class="hljs-comment">#带L1惩罚项的逻辑回归作为基模型的特征选择   </span><br>SelectFromModel(LogisticRegression(penalty=<span class="hljs-string">&quot;l1&quot;</span>, C=<span class="hljs-number">0.1</span>)).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><p>要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><br><span class="hljs-comment">#带L1和L2惩罚项的逻辑回归作为基模型的特征选择   </span><br><span class="hljs-comment">#参数threshold为权值系数之差的阈值   </span><br>SelectFromModel(LR(threshold=<span class="hljs-number">0.5</span>, C=<span class="hljs-number">0.1</span>)).fit_transform(iris.data, iris.target)<br></code></pre></td></tr></table></figure><ul><li>基于学习模型的特征排序 这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。通过这种训练对特征进行打分获得相关性后再训练最终模型。</li></ul><p>在<a href="https://link.zhihu.com/?target=https%3A//archive.ics.uci.edu/ml/datasets/Housing">波士顿房价数据集</a>上使用sklearn的<a href="https://link.zhihu.com/?target=https%3A//scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">随机森林回归</a>给出一个<strong>单变量选择</strong>的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> cross_val_score, ShuffleSplit<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_boston<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestRegressor<br><br><span class="hljs-comment">#加载波士顿房价作为数据集</span><br>boston = load_boston()<br>X = boston[<span class="hljs-string">&quot;data&quot;</span>]<br>Y = boston[<span class="hljs-string">&quot;target&quot;</span>]<br>names = boston[<span class="hljs-string">&quot;feature_names&quot;</span>]<br><br><span class="hljs-comment">#n_estimators为森林中树木数量，max_depth树的最大深度</span><br>rf = RandomForestRegressor(n_estimators=<span class="hljs-number">20</span>, max_depth=<span class="hljs-number">4</span>)<br>scores = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">1</span>]):<br>    <span class="hljs-comment">#每次选择一个特征，进行交叉验证，训练集和测试集为7:3的比例进行分配，</span><br>    <span class="hljs-comment">#ShuffleSplit()函数用于随机抽样（数据集总数，迭代次数，test所占比例）</span><br>    score = cross_val_score(rf, X[:, i:i+<span class="hljs-number">1</span>], Y, scoring=<span class="hljs-string">&quot;r2&quot;</span>,<br>                               cv=ShuffleSplit(<span class="hljs-built_in">len</span>(X), <span class="hljs-number">3</span>, <span class="hljs-number">.3</span>))<br>    scores.append((<span class="hljs-built_in">round</span>(np.mean(score), <span class="hljs-number">3</span>), names[i]))<br><br><span class="hljs-comment">#打印出各个特征所对应的得分</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">sorted</span>(scores, reverse=<span class="hljs-literal">True</span>))<br></code></pre></td></tr></table></figure><p>输出结果：</p><p>[(0.64300000000000002, 'LSTAT'), (0.625, 'RM'), (0.46200000000000002, 'NOX'), (0.373, 'INDUS'), (0.30299999999999999, 'TAX'), (0.29799999999999999, 'PTRATIO'), (0.20399999999999999, 'RAD'), (0.159, 'CRIM'), (0.14499999999999999, 'AGE'), (0.097000000000000003, 'B'), (0.079000000000000001, 'ZN'), (0.019, 'CHAS'), (0.017999999999999999, 'DIS')]</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-selection%EF%BC%89/">#特征选择（Feature selection）</a></div></div><div class="license-box my-3"><div class="license-title"><div>特征选择（Feature selection）方法汇总</div><div>http://hexiaobo.xyz/特征选择（Feature-selection）方法汇总</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>He Xiaobo</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年11月14日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/ML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9A%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB" title="ML学习笔记#4 逻辑回归：二分类到多分类"><span class="hidden-mobile">ML学习笔记#4 逻辑回归：二分类到多分类</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"WPuy4XE9P8wlHWjSI0aTy5K5-gzGzoHsz","appKey":"z2XSrNfyjSJL6jScFig6DYeG","path":"window.location.pathname","placeholder":"说点什么吧","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="http://hexiaobo.xyz" target="_blank" rel="nofollow noopener"><span>Future</span></a> <i class="iconfont icon-love"></i> <a href="http://hexiaobo.xyz" target="_blank" rel="nofollow noopener"><span>Tech</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div><div class="beian"><span><a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">豫ICP备2021031457号</a></span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>